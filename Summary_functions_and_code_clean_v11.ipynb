{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions and code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M. Emile F. Apol**, *Hanze University of Applied Sciences, Groningen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest update: 2022-11-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of functions and code:\n",
    "\n",
    "1. **DS_Q_Q_Plot**:           Make a Q-Q plot for a normal distribution with 95% CI lines\n",
    "2. **DS_Q_Q_Hist**:           Make a histogram with fitted normal distribution\n",
    "3. **DS_OLS_AIC**:            Calculate the ${AIC}$ and ${AIC_c}$-values from a statsmodels.api OLS fit\n",
    "4. **DS_OLS_predict_with_CI**: Calculate the predicted $y$-values with CI from a statsmodels.api OLS fit\n",
    "5. **DS_Mu_r**:               Calculate the central sample moment $m_r$  from a 1d array\n",
    "6. **fsolve**:             Numerically solving one ML equation (1 parameter)\n",
    "7. **fsolve**:            Numerically solving two ML equations (2 parameters)\n",
    "8. **DS_1sample_ztest_means**:  1-sample $z$-test for means (1- and 2-sided, known standard deviation)\n",
    "9. **DS_2sample_ztest_means**:  2-sample $z$-test for means (1- and 2-sided, known standard deviations)\n",
    "10. **DS_paired_ztest_means**:   Paired $z$-test (1- and 2-sided, known standard deviation of differences)\n",
    "11. **DS_1sample_ttest_means**:   1-sample $t$-test (1- and 2-sided)\n",
    "12. **DS_2sample_ttest_means**:   2-sample (Welch's) $t$-test (1- and 2-sided)\n",
    "13. **DS_paired_ttest_means**:    Paired $t$-test (1- and 2-sided)\n",
    "14. **DS_1sample_ztest_props**: 1-sample $z$-test for proportions (1- and 2-sided)\n",
    "15. **DS_2sample_ztest_props**: 2-sample $z$-test for proportions (1- and 2-sided)\n",
    "16. **DS_1sample_chi2test_vars**: 1-sample $\\chi^2$-test for variances (1- and 2-sided)\n",
    "17. **DS_2sample_Ftest_vars**:     2-sample $F$-test for variances (1- and 2-sided), assuming normal distribution\n",
    "18. **DS_2sample_Levenetest_vars**:  2-sample Levene-Brown-Forsythe-test for variances (1- and 2-sided)\n",
    "19. **DS_xtab**:            Make a 1d or 2d contingency table from 1 or 2 arrays\n",
    "20. **DS_beta_nbinom.pmf**, \n",
    "    **DS_beta_nbinom.cdf**, \n",
    "    **DS_beta_nbinom.ppf**, \n",
    "    **DS_beta_nbinom.rvs**: The Beta-Negative Binomial distribution (not yet in scipy.stats)\n",
    "21. **DS_CalibrationAnalysis**:  Analysis of calibration models: fitting, model selection, plotting, interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Make a Q-Q plot for a normal distribution with 95% CI lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_Q_Q_Plot(y, est = 'robust', **kwargs):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_Q_Q_Plot(y, est = 'robust', **kwargs)\n",
    "    \n",
    "       This function makes a normal quantile-quantile plot (Q-Q-plot), also known\n",
    "       as a probability plot, to visually check whether data follow a normal distribution.\n",
    "    \n",
    "    Requires:            - \n",
    "    \n",
    "    Arguments:\n",
    "      y                  data array\n",
    "      est                Estimation method for normal parameters mu and sigma:\n",
    "                         either 'robust' (default), or 'ML' (Maximum Likelihood),\n",
    "                         or 'preset' (given values)\n",
    "      N.B. If est='preset' than the *optional* parameters mu, sigma must be provided:\n",
    "      mu                 preset value of mu\n",
    "      sigma              preset value of sigma\n",
    "      \n",
    "    Returns:\n",
    "      Estimated mu, sigma, n, and expected number of datapoints outside CI in Q-Q-plot.\n",
    "      Q-Q-plot\n",
    "      \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2020-01-06, revision 2022-08-30\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import iqr # iqr is the Interquartile Range function\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # First, get the optional arguments mu and sigma:\n",
    "    mu_0 = kwargs.get('mu', None)\n",
    "    sigma_0 = kwargs.get('sigma', None)\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    # Calculate order statistic:\n",
    "    y_os = np.sort(y)\n",
    "  \n",
    "    # Estimates of mu and sigma:\n",
    "    # ML estimates:\n",
    "    mu_ML = np.mean(y)\n",
    "    sigma2_ML = np.var(y)\n",
    "    sigma_ML = np.std(y) # biased estimate\n",
    "    s2 = np.var(y, ddof=1)\n",
    "    s = np.std(y, ddof=1) # unbiased estimate\n",
    "    # Robust estimates:\n",
    "    mu_R = np.median(y)\n",
    "    sigma_R = iqr(y)/1.349\n",
    "\n",
    "    # Assign values of mu and sigma for z-transform:\n",
    "    if est == 'ML':\n",
    "        mu, sigma = mu_ML, s\n",
    "    elif est == 'robust':\n",
    "        mu, sigma = mu_R, sigma_R\n",
    "    elif est == 'preset':\n",
    "        mu, sigma = mu_0, sigma_0\n",
    "    else:\n",
    "        print('Wrong estimation method chosen!')\n",
    "        return()\n",
    "        \n",
    "    print('Estimation method: ' + est)\n",
    "    print('n = {:d}, mu = {:.4g}, sigma = {:.4g}'.format(n, mu,sigma))\n",
    "    \n",
    "    # Expected number of deviations (95% confidence level):\n",
    "    n_dev = np.round(0.05*n)\n",
    "    \n",
    "    print('Expected number of data outside CI: {:.0f}'.format(n_dev))\n",
    "         \n",
    "    # Perform z-transform: sample quantiles z.i\n",
    "    z_i = (y_os - mu)/sigma\n",
    "\n",
    "    # Calculate cumulative probabilities p.i:\n",
    "    i = np.array(range(n)) + 1\n",
    "    p_i = (i - 0.5)/n\n",
    "\n",
    "    # Calculate theoretical quantiles z.(i):\n",
    "    from scipy.stats import norm\n",
    "    z_th = norm.ppf(p_i, 0, 1)\n",
    "\n",
    "    # Calculate SE or theoretical quantiles:\n",
    "    SE_z_th = (1/norm.pdf(z_th, 0, 1)) * np.sqrt((p_i * (1 - p_i)) / n)\n",
    "\n",
    "    # Calculate 95% CI of diagonal line:\n",
    "    CI_upper = z_th + 1.96 * SE_z_th\n",
    "    CI_lower = z_th - 1.96 * SE_z_th\n",
    "\n",
    "    # Make Q-Q plot:\n",
    "    plt.plot(z_th, z_i, 'o', color='k', label='experimental data')\n",
    "    plt.plot(z_th, z_th, '--', color='r', label='normal line')\n",
    "    plt.plot(z_th, CI_upper, '--', color='b', label='95% CI')\n",
    "    plt.plot(z_th, CI_lower, '--', color='b')\n",
    "    plt.xlabel('Theoretical quantiles, $z_{(i)}$')\n",
    "    plt.ylabel('Sample quantiles, $z_i$')\n",
    "    plt.title('Q-Q plot (' + est + ')')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Make a histogram with fitted normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_Q_Q_Hist(y, est='robust', **kwargs):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_Q_Q_Hist(y, est='robust', **kwargs)\n",
    "    \n",
    "       This function makes a histogram of the data and superimposes a fitted normal\n",
    "       distribution.\n",
    "       \n",
    "    Requires:            - \n",
    "    \n",
    "    Arguments:\n",
    "      y                  data array\n",
    "      est                Estimation method for normal parameters mu and sigma:\n",
    "                         either 'robust' (default), or 'ML' (Maximum Likelihood),\n",
    "                         or 'preset' (given values)\n",
    "      N.B. If est='preset' than the optional parameters mu, sigma must be provided:\n",
    "      mu                 preset value of mu\n",
    "      sigma              preset value of sigma\n",
    "    \n",
    "    Returns:\n",
    "      Estimations of mu and sigma\n",
    "      Histogram of data with estimated normal distribution superimposed\n",
    "      \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2020-01-06\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import iqr # iqr is the Interquartile Range function\n",
    "    from scipy.stats import norm\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # First, get the optional arguments mu and sigma:\n",
    "    mu_0 = kwargs.get('mu', None)\n",
    "    sigma_0 = kwargs.get('sigma', None)\n",
    "    \n",
    "    n = len(y)\n",
    "    \n",
    "    # Estimates of mu and sigma:\n",
    "    # ML estimates:\n",
    "    mu_ML = np.mean(y)\n",
    "    sigma2_ML = np.var(y) # biased estimate\n",
    "    sigma_ML = np.std(y) \n",
    "    s2 = np.var(y, ddof=1) # unbiased estimate\n",
    "    s = np.std(y, ddof=1) \n",
    "    # Robust estimates:\n",
    "    mu_R = np.median(y)\n",
    "    sigma_R = iqr(y)/1.349\n",
    "\n",
    "    # Assign values of mu and sigma for z-transform:\n",
    "    if est == 'ML':\n",
    "        mu, sigma = mu_ML, s       \n",
    "    elif est == 'robust':\n",
    "        mu, sigma = mu_R, sigma_R\n",
    "    elif est == 'preset':\n",
    "        mu, sigma = mu_0, sigma_0\n",
    "    else:\n",
    "        print('Wrong estimation method chosen!')\n",
    "        return()\n",
    "    print('Estimation method: ' + est)\n",
    "    print('mu = {:.4g}, sigma = {:.4g}'.format(mu,sigma))\n",
    "        \n",
    "    # Calculate the CLT normal distribution:\n",
    "    x = np.linspace(np.min(y), np.max(y), 501)\n",
    "    rv = np.array([norm.pdf(xi, loc = mu, scale = sigma) for xi in x])\n",
    "    \n",
    "    # Make a histogram with corresponding normal distribution:\n",
    "    plt.hist(x=y, density=True, bins='auto', \n",
    "             color='darkgrey',alpha=1, rwidth=1, label='experimental')\n",
    "    plt.plot(x, rv, 'r', label='normal approximation')\n",
    "    plt.grid(axis='y', alpha=0.5)\n",
    "    plt.xlabel('Values, $y$')\n",
    "    plt.ylabel('Probability $f(y)$')\n",
    "    plt.title('Histogram with corresponding normal distribution (' + est + ')')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate the ${\\rm AIC}$ and ${\\rm AIC_c}$-values from a statsmodels.api OLS fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_OLS_AIC(results):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_OLS_AIC(results)\n",
    "    \n",
    "      This function calculates the Akaike AIC and small-sample Akaike AIC.c-values \n",
    "      of a LS linear regression model based on the object from a statsmodels.api.OLS.fit().\n",
    "     \n",
    "      Equations:\n",
    "     \n",
    "      AIC = n*log(SS.err/n) + 2*(P+1)\n",
    "      AIC.c = n*log(SS.err/n) + 2*(P+1) + 2*(P+1)*(P+2)/(n-P-2)\n",
    "      \n",
    "      where n = number of observations, P = number of model parameters, and \n",
    "         SS.err = residual sum of squares\n",
    "         \n",
    "    Requires:      numpy as np \n",
    "    \n",
    "    Input:         results       results of a statsmodels.api.OLS.fit()\n",
    "    \n",
    "    Return:        AIC, AIC.c    Akaike value, small-sample Akaike value\n",
    "                   N.B. If n-P-2 < 1, AIC.c will return np.nan\n",
    "    \n",
    "    Author:        M.E.F. Apol\n",
    "    Date:          2020-01-06, update: 2022-10-31\n",
    "    \"\"\"\n",
    "    \n",
    "    SS_err = results.ssr\n",
    "    n = results.nobs\n",
    "    P = len(results.params)\n",
    "    AIC = n * np.log(SS_err/n) + 2 * (P + 1)\n",
    "    if(n-P-2 >0):\n",
    "        AIC_c = AIC + 2 * (P + 1) * (P + 2) / (n - P - 2)\n",
    "    else:\n",
    "        AIC_c = np.nan\n",
    "    return(AIC, AIC_c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate the predicted $y$-values with CI from a statsmodels.api OLS fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_OLS_predict_with_CI(results, X, confidence=0.95):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_OLS_predict_with_CI(results, X, confidence=0.95)\n",
    "    \n",
    "      This function calculates the model predictions y_pred with \n",
    "      (100*confidence)% Confidence Interval (lower and upper limits) \n",
    "      of a LS linear regression model based on the object from a statsmodels.api.OLS.fit().\n",
    "    \n",
    "    Requires:           numpy as np\n",
    "    \n",
    "    Input: \n",
    "        results         results of a statsmodels.api.OLS.fit()\n",
    "        X               design matrix for model using predicting x-values\n",
    "        confidence=0.95 confidence level of CI [default 95%]\n",
    "    \n",
    "    Return:\n",
    "        y_pred          predicted y-value\n",
    "        y_pred_lower    lower confidence*100% CI\n",
    "        y_pred_upper    upper confidence*100% CI\n",
    "     \n",
    "    Author: M.E.F. Apol\n",
    "    Date:   30-10-2022\n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    \n",
    "    a_hat = results.params\n",
    "    P = len(a_hat)\n",
    "    V = results.cov_params()\n",
    "    n = results.nobs\n",
    "    \n",
    "    y_pred = []\n",
    "    y_pred_lower = []\n",
    "    y_pred_upper = []\n",
    "    t_vals = t.ppf([0.025, 0.975], n-P)\n",
    "\n",
    "    # Be sure that X is a 2d array:\n",
    "    if (np.ndim(X) == 1):\n",
    "        X = np.reshape(X, (len(X), 1))\n",
    "    n_plot = len(X[:,0])\n",
    "        \n",
    "    # Calculate the prediction of the model with 100*confidence% CI:\n",
    "    for i in range(n_plot):\n",
    "        x_p = X[i,:]\n",
    "        y_p = np.dot(x_p, a_hat)\n",
    "        s_y_p = np.sqrt(np.linalg.multi_dot((x_p, V, x_p)))\n",
    "        y_p_lower, y_p_upper = y_p + t_vals * s_y_p\n",
    "        y_pred.append(y_p)\n",
    "        y_pred_lower.append(y_p_lower)\n",
    "        y_pred_upper.append(y_p_upper)\n",
    "        \n",
    "    return(y_pred, y_pred_lower, y_pred_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate the central sample moment $m_r$ from a 1d array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_Mu_r(y, r):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_Mu_r(y, r)\n",
    "     \n",
    "       This function calculates m.r, the r.th central moment of a 1d numpy array y.\n",
    "    \n",
    "    Requires:          numpy as np\n",
    "     \n",
    "    Input:\n",
    "      y                data vector\n",
    "      r                power\n",
    "     \n",
    "    Return:            mu.r[y] : the rth central moment of y\n",
    "    \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2020-01-06\n",
    "    \"\"\"\n",
    "    \n",
    "    y_av = np.mean(y)\n",
    "    mu_r = np.mean((y - y_av)**r)\n",
    "    return(mu_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Numerically solving one ML equation (1 parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Numerically solving two ML equations (2 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 1-sample $z$-test for means (1- and 2-sided, with known standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_1sample_ztest_means(y, sigma, popmean=0, alternative='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_1sample_ztest_means(y, sigma, popmean=0, alternative='two-sided', alpha=0.05)\n",
    "     \n",
    "        This function performs a 1-sample z-test (Null Hypothesis Significance Test) \n",
    "        in the spirit of R, testing 1 average with *known* standard deviation.\n",
    "        The function also evaluates the effect size (Cohen's d).\n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-27, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_1sample_ztest_means(y, sigma = sigma, popmean = mu*, \n",
    "                            alternative=['two-sided']/'less'/'greater', alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu != mu*\n",
    "                                       'less'                 H1: mu < mu*\n",
    "                                       'greater'              H1: mu > mu*\n",
    "                         sigma: *known* standard deviation of dataset\n",
    "                         alpha: significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            z, p-value, z.crit.L, z.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where z.crit.L and z.crit.R are the lower and upper critical values, \n",
    "                       z is the test statistic and p-value is the p-value of the test.\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    n = len(y)\n",
    "    y_av = np.mean(y)\n",
    "    z = (y_av - popmean)/(sigma / np.sqrt(n))\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('1-sample z-test for 1 mean:')\n",
    "    print('     assuming Normal(mu | sigma2) data for dataset')\n",
    "    print('y.av = {:.3g}, mu* = {:.3g}, sigma = {:.3g}, n = {:d}, alpha = {:.3g}'.format(y_av, popmean, sigma, n, alpha))\n",
    "    print('H0: mu  = mu*')\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu != mu*')\n",
    "        p_value = 2 * norm.cdf(-np.abs(z), 0, 1)\n",
    "        z_crit_L = norm.ppf(alpha/2, 0, 1)\n",
    "        z_crit_R = norm.ppf(1-alpha/2, 0, 1)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu  < mu*')\n",
    "        p_value = norm.cdf(z)\n",
    "        z_crit_L = norm.ppf(alpha, 0, 1)\n",
    "        z_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu  > mu*')\n",
    "        p_value = 1 - norm.cdf(z, 0, 1)\n",
    "        # better precision, use the survival function:\n",
    "        p_value = norm.sf(z, 0, 1)\n",
    "        z_crit_L = float('-inf')\n",
    "        z_crit_R = norm.ppf(1-alpha, 0, 1)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        z, p_value, z_crit_L, z_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(z, p_value, z_crit_L, z_crit_R)\n",
    "    \n",
    "    # Effect size (Cohen's d.s):\n",
    "    d_s = z * np.sqrt(1/n)\n",
    "    print('z = {:.4g}, p-value = {:.4g}, z.crit.L = {:.4g}, z.crit.R = {:.4g}'.format(z, p_value, z_crit_L, z_crit_R))\n",
    "    print('Effect size: d.s = {:.3g}; benchmarks |d.s|: 0.2 = small, 0.5 = medium, 0.8 = large'.format(d_s))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(z, p_value, z_crit_L, z_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.  2-sample $z$-test for means (1- and 2-sided, with known standard deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_2sample_ztest_means(y1, y2, sigma1, sigma2, alternative ='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_2sample_ztest_means(y1, y2, sigma1, sigma2, alternative ='two-sided', alpha=0.05)\n",
    "     \n",
    "        This function performs a 2-sample z-test (Null Hypothesis Significance Test)\n",
    "        in the spirit of R, testing 2 averages with *known* standard deviations.\n",
    "        The function also evaluates the effect size (Cohen's d).\n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_2sample_ztest_means(y1, y2, sigma1, sigma2, \n",
    "                              alternative=['two-sided']/'less'/'greater', alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu1 != mu_2\n",
    "                                       'less'                 H1: mu1 < mu2\n",
    "                                       'greater'              H1: mu1 > mu2\n",
    "                         sigma1, sigma2: *known* standard deviations of datasets 1 and 2\n",
    "                         alpha: significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            z, p-value, z.crit.L, z.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where z.crit.L and z.crit.R are the lower and upper critical values, \n",
    "                       z is the test statistic and p-value is the p-value of the test. \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-27, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    n_1 = len(y1) ; n_2 = len(y2)\n",
    "    y_av_1 = np.mean(y1) ; y_av_2 = np.mean(y2)\n",
    "    z = (y_av_1 - y_av_2)/np.sqrt( sigma1**2/n_1 + sigma2**2/n_2 )\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('2-sample z-test for 2 means:')\n",
    "    print('     assuming Normal(mu.1 | sigma2.1) data for dataset 1')\n",
    "    print('     assuming Normal(mu.2 | sigma2.2) data for dataset 2')\n",
    "    print('y.av.1 = {:.3g}, y.av.2 = {:.3g}, sigma.1 = {:.3g}, sigma.2 = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(y_av_1, y_av_2, sigma1, sigma2, n_1, n_2, alpha))\n",
    "    print('H0: mu.1  = mu.2')\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu.1 != mu.2')\n",
    "        p_value = 2 * norm.cdf(-np.abs(z), 0, 1)\n",
    "        z_crit_L = norm.ppf(alpha/2, 0, 1)\n",
    "        z_crit_R = norm.ppf(1-alpha/2, 0, 1)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu.1  < mu.2')\n",
    "        p_value = norm.cdf(z)\n",
    "        z_crit_L = norm.ppf(alpha, 0, 1)\n",
    "        z_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu.1  > mu.2')\n",
    "        p_value = 1 - norm.cdf(z, 0, 1)\n",
    "        # better precision, use the survival function:\n",
    "        p_value = norm.sf(z, 0, 1)\n",
    "        z_crit_L = float('-inf')\n",
    "        z_crit_R = norm.ppf(1-alpha, 0, 1)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        z, p_value, z_crit_L, z_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(z, p_value, z_crit_L, z_crit_R)\n",
    "    \n",
    "    # Effect size (Cohen's d.av):\n",
    "    sigma_pooled = np.sqrt((n_1*sigma1**2 + n_2*sigma2**2)/(n_1 + n_2))\n",
    "    d_av = (y_av_1 - y_av_2)/sigma_pooled\n",
    "    print('z = {:.4g}, p-value = {:.4g}, z.crit.L = {:.4g}, z.crit.R = {:.4g}'.format(z, p_value, z_crit_L, z_crit_R))\n",
    "    print('Effect size: d.av = {:.3g}; benchmarks |d.av|: 0.2 = small, 0.5 = medium, 0.8 = large'.format(d_av))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(z, p_value, z_crit_L, z_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Paired $z$-test for means (1- and 2-sided, with known standard deviation of the differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_paired_ztest_means(y1, y2, sigma_d, alternative ='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_paired_ztest_means(y1, y2, sigma_d, alternative ='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a paired z-test (Null Hypothesis Significance Test)\n",
    "       in the spirit of R, testing the average difference between two sets of *paired* data \n",
    "       with *known* standard deviation of the difference, sigma_d.\n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_paired_ztest_means(y1, y2, sigma_d = sigma, \n",
    "                           alternative=['two-sided']/'less'/'greater', alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu != mu*\n",
    "                                       'less'                 H1: mu < mu*\n",
    "                                       'greater'              H1: mu > mu*\n",
    "                         sigma_d: known standard deviation of the differences\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            z, p-value, z.crit.L, z.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where z.crit.L and z.crit.R are the lower and upper critical values, \n",
    "                       z is the test statistic and p-value is the p-value of the test. \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-27, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    # Error checking:\n",
    "    if len(y1) != len(y2):\n",
    "        print('Error: Datasets of unequal length...')\n",
    "        return\n",
    "    \n",
    "    y_av_1 = np.mean(y1); y_av_2 = np.mean(y2)\n",
    "    d_av = y_av_1 - y_av_2\n",
    "    n_d = len(y1)\n",
    "    z = (d_av)/(sigma_d / np.sqrt(n_d))\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('Paired z-test for 2 means:')\n",
    "    print('     assuming Normal(mu.d |sigma2.d) data for difference between datasets 1 and 2')\n",
    "    print('y.av.1 = {:.3g}, y.av.2 = {:.3g}, sigma.d = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(y_av_1, y_av_2, sigma_d, n_d, n_d, alpha))\n",
    "    print('H0: mu.1  = mu.2')\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu.1 != mu.2')\n",
    "        p_value = 2 * norm.cdf(-np.abs(z), 0, 1)\n",
    "        z_crit_L = norm.ppf(alpha/2, 0, 1)\n",
    "        z_crit_R = norm.ppf(1-alpha/2, 0, 1)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu.1  < mu.2')\n",
    "        p_value = norm.cdf(z)\n",
    "        z_crit_L = norm.ppf(alpha, 0, 1)\n",
    "        z_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu.1  > mu.2')\n",
    "        p_value = 1 - norm.cdf(z, 0, 1)\n",
    "        # better precision, use the survival function:\n",
    "        p_value = norm.sf(z, 0, 1)\n",
    "        z_crit_L = float('-inf')\n",
    "        z_crit_R = norm.ppf(1-alpha, 0, 1)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        z, p_value, z_crit_L, z_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(z, p_value, z_crit_L, z_crit_R)\n",
    "    print('z = {:.4g}, p-value = {:.4g}, z.crit.L = {:.4g}, z.crit.R = {:.4g}'.format(z, p_value, z_crit_L, z_crit_R))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(z, p_value, z_crit_L, z_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 1-sample $t$-test (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_1sample_ttest_means(y, popmean=0, alternative = 'two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_1sample_ttest_means(y, popmean=0, alternative = 'two-sided', alpha=0.05)\n",
    "    \n",
    "       This function performs a 1-sample t-test (Null Hypothesis Significance Test) \n",
    "       in the spirit of R, testing 1 average with *unknown* standard deviation.\n",
    "       The function also evaluates the effect size (Cohen's d).\n",
    "       \n",
    "    Requires:          -\n",
    "     \n",
    "    Usage:             DS_1sample_ttest_means(y, popmean = mu*, \n",
    "                            alternative=['two-sided']/'less'/'greater', \n",
    "                            alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu != mu*\n",
    "                                       'less'                 H1: mu < mu*\n",
    "                                       'greater'              H1: mu > mu*\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            t, p-value, t.crit.L, t.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where t.crit.L and t.crit.R are the lower and upper critical values, \n",
    "                       t is the test statistic and p-value is the p-value of the test.\n",
    "                       \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-27, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import ttest_1samp\n",
    "    from scipy.stats import t as t_distr\n",
    "    import numpy as np\n",
    "\n",
    "    t, p_samp = ttest_1samp(y, popmean)\n",
    "    print(80*'-')\n",
    "    print('1-sample t-test for 1 mean:')\n",
    "    print('     assuming Normal(mu, sigma2) data for dataset')\n",
    "    y_av = np.mean(y)\n",
    "    n = len(y)\n",
    "    df = n - 1\n",
    "    s2 = np.var(y, ddof=1)\n",
    "    print('y.av = {:.3g}, mu* = {:.3g}, s2 = {:.3g}, n = {:d}, alpha = {:.3g}'.format(y_av, popmean, s2, n, alpha))\n",
    "    print('H0: mu  = mu*')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu != mu*')\n",
    "        p_value = p_samp\n",
    "        t_crit_L = t_distr.ppf(alpha/2, df)\n",
    "        t_crit_R = t_distr.ppf(1-alpha/2, df)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu  < mu*')\n",
    "        if t <= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = t_distr.ppf(alpha, df)\n",
    "        t_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu  > mu*')\n",
    "        if t >= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = float('-inf')\n",
    "        t_crit_R = t_distr.ppf(1-alpha, df)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        t, p_value, t_crit_L, t_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(t, p_value, t_crit_L, t_crit_R)\n",
    "    # Effect size:\n",
    "    d_s = t * np.sqrt(1/n)\n",
    "    print('t = {:.4g}, p-value = {:.4g}, t.crit.L = {:.4g}, t.crit.R = {:.4g}, df = {:.4g}'.format(t, p_value, t_crit_L, t_crit_R, df))\n",
    "    print('Effect size: d.s = {:.3g}; benchmarks |d.s|: 0.2 = small, 0.5 = medium, 0.8 = large'.format(d_s))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(t, p_value, t_crit_L, t_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 2-sample (Welch's) $t$-test (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_2sample_ttest_means(y1, y2, equal_var=False, alternative='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_2sample_ttest_means(y1, y2, equal_var=False, alternative='two-sided', alpha=0.05)\n",
    "    \n",
    "       This function performs a 2-sample (Welch's) t-test (Null Hypothesis Significance Test) \n",
    "       in the spirit of R, testing 2 averages with *unknown* standard deviation.\n",
    "       The function also evaluates the effect size (Cohen's d).\n",
    "       \n",
    "    Requires:          -\n",
    "       \n",
    "    Usage:             DS_2sample_ttest_means(y1, y2, \n",
    "                            alternative=['two-sided']/'less'/'greater',\n",
    "                            equal_var=[False]/True, alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu_1 != mu_2\n",
    "                                       'less'                 H1: mu_1 < mu_2\n",
    "                                       'greater'              H1: mu_1 > mu_2\n",
    "                         equal_var = False                    perform Welch t-test\n",
    "                                     True                     perform 2-sample t-test\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            t, p-value, t.crit.L, t.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where t.crit.L and t.crit.R are the lower and upper critical values, \n",
    "                       t is the test statistic and p-value is the p-value of the test.     \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-28, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    from scipy.stats import t as t_distr\n",
    "    import numpy as np\n",
    "    \n",
    "    t, p_samp = ttest_ind(y1, y2, equal_var = equal_var)\n",
    "    y_av_1 = np.mean(y1)\n",
    "    y_av_2 = np.mean(y2)\n",
    "    n_1 = len(y1)\n",
    "    n_2 = len(y2)\n",
    "    s2_1 = np.var(y1, ddof=1)\n",
    "    s2_2 = np.var(y2, ddof=1)\n",
    "    print(80*'-')\n",
    "    if equal_var == True:\n",
    "        print('2-sample t-test for 2 means:')\n",
    "        print('     assuming Normal(mu.1, sigma2) data for dataset 1')\n",
    "        print('     assuming Normal(mu.2, sigma2) data for dataset 2')\n",
    "        df = n_1 + n_2 - 2\n",
    "    else:\n",
    "        print('Welch t-test for 2 means:')\n",
    "        df = (s2_1/n_1 + s2_2/n_2)**2 / ( 1/(n_1-1)*(s2_1/n_1)**2 + 1/(n_2-1)*(s2_2/n_2)**2 )\n",
    "        print('     assuming Normal(mu.1, sigma2.1) data for dataset 1')\n",
    "        print('     assuming Normal(mu.2, sigma2.2) data for dataset 2')\n",
    "    print('y.av.1 = {:.3g}, y.av.2 = {:.3g}, s2.1 = {:.3g}, s2.2 = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(y_av_1, y_av_2, s2_1, s2_2, n_1, n_2, alpha))\n",
    "    print('H0: mu.1  = mu.2')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu.1 != mu.2')\n",
    "        p_value = p_samp\n",
    "        t_crit_L = t_distr.ppf(alpha/2, df)\n",
    "        t_crit_R = t_distr.ppf(1-alpha/2, df)      \n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu.1  < mu.2')\n",
    "        if t <= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = t_distr.ppf(alpha, df)\n",
    "        t_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu.1  > mu.2')\n",
    "        if t >= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = float('-inf')\n",
    "        t_crit_R = t_distr.ppf(1-alpha, df)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        t, p_value, t_crit_L, t_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(t, p_value, t_crit_L, t_crit_R)\n",
    "    \n",
    "    # Effect size (Cohen's d.av):\n",
    "    d_av = t * np.sqrt(1/n_1 + 1/n_2)\n",
    "    print('t = {:.4g}, p-value = {:.4g}, t.crit.L = {:.4g}, t.crit.R = {:.4g}, df = {:.4g}'.format(t, p_value, t_crit_L, t_crit_R, df))\n",
    "    print('Effect size: d.av = {:.3g}; benchmarks |d.av|: 0.2 = small, 0.5 = medium, 0.8 = large'.format(d_av))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(t, p_value, t_crit_L, t_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Paired $t$-test (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_paired_ttest_means(y1, y2, alternative='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_paired_ttest_means(y1, y2, alternative='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a paired t-test (Null Hypothesis Significance Test) \n",
    "       in the spirit of R, testing 2 averages of paired data with *unknown* standard deviation.\n",
    "       The function also evaluates the effect size (Cohen's d).\n",
    "       \n",
    "    Requires:          -\n",
    "       \n",
    "    Usage:             DS_paired_ttest_means(y1, y2, \n",
    "                            alternative=['two-sided']/'less'/'greater', \n",
    "                            alpha = 0.05)\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: mu_1 != mu_2\n",
    "                                       'less'                 H1: mu_1 < mu_2\n",
    "                                       'greater'              H1: mu_1 > mu_2\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            t, p-value, t.crit.L, t.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where t.crit.L and t.crit.R are the lower and upper critical values, \n",
    "                       t is the test statistic and p-value is the p-value of the test.     \n",
    "    \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2020-11-11, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import ttest_rel\n",
    "    from scipy.stats import t as t_distr\n",
    "    import numpy as np\n",
    "    \n",
    "    # Error checking:\n",
    "    if len(y1) != len(y2):\n",
    "        print('Error: Datasets of unequal length...')\n",
    "        return\n",
    "    \n",
    "    t, p_samp = ttest_rel(y1, y2)\n",
    "    y_av_1 = np.mean(y1)\n",
    "    y_av_2 = np.mean(y2)\n",
    "    d_av = y_av_1 - y_av_2\n",
    "    n_d = len(y1)\n",
    "    df = n_d - 1\n",
    "    s2_1 = np.var(y1, ddof=1)\n",
    "    s2_2 = np.var(y2, ddof=1)\n",
    "    print(80*'-')\n",
    "    print('Paired t-test for 2 means:')\n",
    "    print('     assuming Normal(mu.d, sigma2.d) data for difference between datasets 1 and 2')\n",
    "    print('y.av.1 = {:.3g}, y.av.2 = {:.3g}, s2.1 = {:.3g}, s2.2 = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(y_av_1, y_av_2, s2_1, s2_2, n_d, n_d, alpha))\n",
    "    print('H0: mu.1  = mu.2')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: mu.1 != mu.2')\n",
    "        p_value = p_samp\n",
    "        t_crit_L = t_distr.ppf(alpha/2, df)\n",
    "        t_crit_R = t_distr.ppf(1-alpha/2, df)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: mu.1  < mu.2')\n",
    "        if t <= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = t_distr.ppf(alpha, df)\n",
    "        t_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: mu.1  > mu.2')\n",
    "        if t >= 0:\n",
    "            p_value = p_samp/2\n",
    "        else:\n",
    "            p_value = 1 - p_samp/2\n",
    "        t_crit_L = float('-inf')\n",
    "        t_crit_R = t_distr.ppf(1-alpha, df)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        t, p_value, t_crit_L, t_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(t, p_value, t_crit_L, t_crit_R)\n",
    "    # Effect size:\n",
    "    d_av = (y_av_1 - y_av_2) / np.sqrt((s2_1 + s2_2)/2)\n",
    "    print('t = {:.4g}, p-value = {:.4g}, t.crit.L = {:.4g}, t.crit.R = {:.4g}, df = {:.4g}'.format(t, p_value, t_crit_L, t_crit_R, df))\n",
    "    print('Effect size: d.av = {:.3g}; benchmarks |d.av|: 0.2 = small, 0.5 = medium, 0.8 = large'.format(d_av))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(t, p_value, t_crit_L, t_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. 1-sample $z$-test for proportions (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_1sample_ztest_props(y, popmean, k=1, alternative ='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_1sample_ztest_props(y, popmean, k=1, alternative ='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a 1-sample z-test (Null Hypothesis Significance Test)\n",
    "       in the spirit of R, testing 1 proportion using a normal approximation, \n",
    "       assuming a Binomial(k, p)-distribution. For Bernoulli data, set k = 1 (default).\n",
    "       The function also evaluates the effect size (Cramer's V2).\n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_1sample_ztest_props(y, popmean = p*, k, \n",
    "                            alternative=['two-sided']/'less'/'greater', alpha = 0.05)\n",
    "     \n",
    "                         Note 1: y is an array with Binomial(k, p) data\n",
    "                         Note 2: If y is an array with BINARY data (0, 1), \n",
    "                                 set k = 1 (Bernoulli data)\n",
    "     \n",
    "                         k:       Binomial(k, p) parameter = number of Bernoulli repetitions\n",
    "                         alternative = 'two-sided' [default]  H1: p != p*\n",
    "                                       'less'                 H1: p < p*\n",
    "                                       'greater'              H1: p > p*\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            z, p-value, z.crit.L, z.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where z.crit.L and z.crit.R are the lower and upper critical values, \n",
    "                       z is the test statistic and p-value is the p-value of the test.    \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-28, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    n = len(y)\n",
    "    p_ML = np.mean(y)/k\n",
    "    p_star = popmean\n",
    "    N = k * n\n",
    "    O_1 = np.sum(y)\n",
    "    O_0 = N - O_1\n",
    "    z = (p_ML - popmean)/np.sqrt(p_star*(1-p_star)/N)\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('1-sample z-test for 1 proportion:')\n",
    "    if k == 1:\n",
    "        print('     assuming Bernoulli(p) data for dataset')\n",
    "    else:\n",
    "        print('     assuming Binomial(' + str(k) + ', p) data for dataset')\n",
    "    print('Observed dataset: O.1 = {:d}, O.0 = {:d}, N = {:d}'.format(O_1, O_0, N))\n",
    "    print('p.ML = {:.3g}, p* = {:.3g}, alpha = {:.3g}'.format(p_ML, popmean, alpha))\n",
    "    print('H0: p  = p*')\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: p != p*')\n",
    "        p_value = 2 * norm.cdf(-np.abs(z), 0, 1)\n",
    "        z_crit_L = norm.ppf(alpha/2, 0, 1)\n",
    "        z_crit_R = norm.ppf(1-alpha/2, 0, 1)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: p  < p*')\n",
    "        p_value = norm.cdf(z)\n",
    "        z_crit_L = norm.ppf(alpha, 0, 1)\n",
    "        z_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: p  > p*')\n",
    "        p_value = 1 - norm.cdf(z, 0, 1)\n",
    "        # better precision, use the survival function:\n",
    "        p_value = norm.sf(z, 0, 1)\n",
    "        z_crit_L = float('-inf')\n",
    "        z_crit_R = norm.ppf(1-alpha, 0, 1)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        z, p_value, z_crit_L, z_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(z, p_value, z_crit_L, z_crit_R)\n",
    "    \n",
    "    # Effect size (Cramer's V2):\n",
    "    V2 = z**2 / N\n",
    "    print('z = {:.4g}, p-value = {:.4g}, z.crit.L = {:.4g}, z.crit.R = {:.4g}'.format(z, p_value, z_crit_L, z_crit_R))\n",
    "    print('Effect size: V2 = {:.3g}; benchmarks V2: 0.01 = small, 0.09 = medium, 0.25 = large'.format(V2))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(z, p_value, z_crit_L, z_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. 2-sample $z$-test for proportions (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_2sample_ztest_props(y1, y2, k1=1, k2=1, alternative='two-sided', alpha=0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_2sample_ztest_props(y1, y2, k1=1, k2=1, alternative='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a 2-sample z-test (Null Hypothesis Significance Test) \n",
    "       in the spirit of R, testing 2 proportions using a normal approximation. \n",
    "       We assume that datset y1 ~ Bin(k1, p1) and y2 ~ Bin(k2, p2).\n",
    "       If we assume that y1 and y2 are binary count data, so y1 ~ Ber(p) and y2 ~ Ber(p), \n",
    "       then set k1 = k2 = 1 (default).\n",
    "       The function also evaluates the effect size (Cramer's V2).\n",
    "        \n",
    "    Requires:          -\n",
    "     \n",
    "    Usage:             DS_2sample_ztest_props(y1, y2, k1, k2, \n",
    "                            alternative=['two-sided']/'less'/'greater', alpha = 0.05)\n",
    "     \n",
    "                         Note 1: y.i (i=1, 2) is an array with Binomial(k.i, p.i) data\n",
    "                         Note 2: If y.i is an array with BINARY data (0, 1), \n",
    "                                 set k.i = 1 (Bernoulli data)\n",
    "     \n",
    "                         k1, k2:    Binomial(k, p) parameter = number of Bernoulli repetitions \n",
    "                                    of datasets y1 and y2\n",
    "     \n",
    "                         alternative = 'two-sided' [default]  H1: p.1 != p.1\n",
    "                                       'less'                 H1: p.1 <  p.2\n",
    "                                       'greater'              H1: p.1 >  p.2\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            z, p-value, z.crit.L, z.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where z.crit.L and z.crit.R are the lower and upper critical values, \n",
    "                       z is the test statistic and p-value is the p-value of the test.    \n",
    "      \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-29, rev. 2022_08_26\n",
    "    Validation:\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    n_1 = len(y1)\n",
    "    n_2 = len(y2)\n",
    "    p_ML_1 = np.mean(y1)/k1\n",
    "    p_ML_2 = np.mean(y2)/k2\n",
    "    O_1 = np.sum(y1)\n",
    "    O_2 = np.sum(y2)\n",
    "    N_1 = k1 * n_1\n",
    "    N_2 = k2 * n_2\n",
    "    N = N_1 + N_2\n",
    "    p_star = (O_1 + O_2)/(N_1 + N_2)\n",
    "    z = (p_ML_1 - p_ML_2)/np.sqrt(p_star*(1-p_star)*(1/N_1 + 1/N_2))\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('2-sample z-test for 2 proportions:')\n",
    "    if k1 == 1:\n",
    "        print('     assuming Bernoulli(p) data for dataset 1')\n",
    "    else:\n",
    "        print('     assuming Binomial(' + str(k1) + ', p) data for dataset 1')\n",
    "    if k2 == 1:\n",
    "        print('     assuming Bernoulli(p) data for dataset 2')\n",
    "    else:\n",
    "        print('     assuming Binomial(' + str(k2) + ', p) data for dataset 2')\n",
    "    print('Observed dataset 1: O.11 = {:d}, O.10 = {:d}, N.1 = {:d}'.format(O_1, N_1-O_1, N_1))\n",
    "    print('Observed dataset 2: O.21 = {:d}, O.20 = {:d}, N.2 = {:d}'.format(O_2, N_2-O_2, N_2))\n",
    "    print('p.ML.1 = {:.3g}, p.ML.2 = {:.3g}, p* = {:.3g}, alpha = {:.3g}'.format(p_ML_1, p_ML_2, p_star, alpha))\n",
    "    print('H0: p.1  = p.2')\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: p.1 != p.2')\n",
    "        p_value = 2 * norm.cdf(-np.abs(z), 0, 1)\n",
    "        z_crit_L = norm.ppf(alpha/2, 0, 1)\n",
    "        z_crit_R = norm.ppf(1-alpha/2, 0, 1)\n",
    "    elif alternative == 'less':\n",
    "        print('H1: p.1  < p.2')\n",
    "        p_value = norm.cdf(z)\n",
    "        z_crit_L = norm.ppf(alpha, 0, 1)\n",
    "        z_crit_R = float('inf')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: p.1  > p.2')\n",
    "        p_value = 1 - norm.cdf(z, 0, 1)\n",
    "        # better precision, use the survival function:\n",
    "        p_value = norm.sf(z, 0, 1)\n",
    "        z_crit_L = float('-inf')\n",
    "        z_crit_R = norm.ppf(1-alpha, 0, 1)\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        z, p_value, z_crit_L, z_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(z, p_value, z_crit_L, z_crit_R)\n",
    "    \n",
    "    # Effect size (Cramer's V2):\n",
    "    V2 = z**2 / N\n",
    "    print('z = {:.4g}, p-value = {:.4g}, z.crit.L = {:.4g}, z.crit.R = {:.4g}'.format(z, p_value, z_crit_L, z_crit_R))\n",
    "    print('Effect size: V2 = {:.3g}; benchmarks V2: 0.01 = small, 0.09 = medium, 0.25 = large'.format(V2))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(z, p_value, z_crit_L, z_crit_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. $\\chi^2$-test for variance (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_1sample_chi2test_vars(y, sigma, alternative = 'two-sided', alpha = 0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_1sample_chi2test_vars(y, sigma, alternative = 'two-sided', alpha = 0.05)\n",
    "         \n",
    "       This function performs a 1-sample chi2-test (Null Hypothesis Significance Test)\n",
    "       in the spirit of R, testing 1 variance sigma^2 (or standard deviation sigma)\n",
    "       using a normal approximation. \n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_1sample_chi2test_vars(y, sigma, \n",
    "                                         alternative=['two-sided']/'less'/'greater', alpha=0.05)\n",
    "                         sigma    reference standard deviation sigma*\n",
    "                         alternative = 'two-sided' [default]  H1: sigma^2 != sigma*^2\n",
    "                                       'less'                 H1: sigma^2 < sigma*^2\n",
    "                                       'greater'              H1: sigma^2 > sigma*^2\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            chi2, p-value, chi2.crit.L, chi2.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where chi2.crit.L and chi2.crit.R are the lower and upper critical values, \n",
    "                       chi2 is the test statistic and p-value is the p-value of the test.    \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-31, rev. 2022_08_26\n",
    "    Validation:        2022-01-31 against Minitab  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from scipy.stats import chi2\n",
    "    import numpy as np\n",
    "        \n",
    "    n = len(y)\n",
    "    y_av = np.mean(y)\n",
    "    s2 = np.var(y, ddof=1)\n",
    "    \n",
    "    print(80*'-')\n",
    "    print('Chi2-test for 1 variance:')\n",
    "    print('     assuming Normal(mu, sigma2) data for dataset')\n",
    "    print('s2 = {:.3g}, sigma*2 = {:.3g}, n = {:d}, alpha = {:.3g}'.format(s2, sigma**2, n, alpha))\n",
    "    print('H0: sigma2  = sigma*2')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: sigma2 != sigma*2')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: sigma2  > sigma*2')\n",
    "    elif alternative == 'less':\n",
    "        print('H1: sigma2  < sigma*2')\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        chi2_samp, p_value, chi2_crit_L, chi2_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(chi2_samp, p_value, chi2_crit_L, chi2_crit_R)\n",
    "        \n",
    "    df = n - 1\n",
    "    chi2_samp = df*s2 / sigma**2\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2*np.min([chi2.cdf(chi2_samp, df), chi2.sf(chi2_samp, df)])\n",
    "        chi2_crit_L = chi2.ppf(alpha/2, df)\n",
    "        chi2_crit_R = chi2.ppf(1-alpha/2, df)\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1-chi2.cdf(chi2_samp, df)\n",
    "        # better precision: use survival function\n",
    "        p_value = chi2.sf(chi2_samp, df)\n",
    "        chi2_crit_L = 0\n",
    "        chi2_crit_R = chi2.ppf(1-alpha, df)\n",
    "    elif alternative == 'less':\n",
    "        p_value = chi2.cdf(chi2_samp, df)\n",
    "        chi2_crit_L = chi2.ppf(alpha, df)\n",
    "        chi2_crit_R = float('inf')\n",
    "    \n",
    "    print('chi2 = {:.4g}, p-value = {:.4g}, chi2.crit.L = {:.4g}, chi2.crit.R = {:.4g}'.format(chi2_samp, p_value, chi2_crit_L, chi2_crit_R))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(chi2_samp, p_value, chi2_crit_L, chi2_crit_R)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. $F$-test for 2 variances (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_2sample_Ftest_vars(y1, y2, alternative = 'two-sided', alpha = 0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_2sample_Ftest_vars(y1, y2, alternative='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a 2-sample F-test (Null Hypothesis Significance Test)\n",
    "       in the spirit of R, testing 2 variances using a normal approximation. \n",
    "       Note that there are *two different conventions* to define the test statistic F:\n",
    "       1) F = (expected) largest variance / (expected) smallest variance, \n",
    "          see Miller & Miller (2005) - Statistics and Chemometrics for Analytical Chemistry. \n",
    "          5th ed. Pearson. \n",
    "          In that case, F >= 1, and the the left critical F-value, F.crit.L is undefined.\n",
    "       2) F = s.1^2 / s.2^2, so F may be smaller or larger then 1.\n",
    "       The output of the function uses convention 1; the output to stdout gives both conventions!\n",
    "       The p-value of the test is - following Fisher - 2 times the upper tail probability \n",
    "       using convention 1.\n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_2sample_Ftest_vars(y1, y2,\n",
    "                                         alternative=['two-sided']/'less'/'greater', alpha=0.05)\n",
    "                         alternative = 'two-sided' [default]  H1: sigma_1^2 != sigma_2^2\n",
    "                                       'less'                 H1: sigma_1^2 < sigma_2^2\n",
    "                                       'greater'              H1: sigma_1^2 > sigma_2^2\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            F, p-value, F.crit.L, F.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where F.crit.L and F.crit.R are the lower and upper critical values, \n",
    "                       F is the test statistic and p-value is the p-value of the test.    \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-01-30, rev. 2022_08_26\n",
    "    Validation:        2022-01-31 against Minitab\n",
    "    \"\"\"\n",
    "        \n",
    "    from scipy.stats import f\n",
    "    import numpy as np\n",
    "        \n",
    "    n_1 = len(y1); n_2 = len(y2)\n",
    "    y_av_1 = np.mean(y1); y_av_2 = np.mean(y2)\n",
    "    s2_1 = np.var(y1, ddof=1); s2_2 = np.var(y2, ddof=1)\n",
    "    \n",
    "    print(80*'-')\n",
    "    \n",
    "    # Perform F-test with convention that F = s2.1 / s2.2\n",
    "    \n",
    "    print('F-test for 2 variances:')\n",
    "    print('     assuming Normal(mu.1, sigma2.1) data for dataset 1')\n",
    "    print('     assuming Normal(mu.2, sigma2.2) data for dataset 2')\n",
    "    print('s2.1 = {:.3g}, s2.2 = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(s2_1, s2_2, n_1, n_2, alpha))\n",
    "    print('H0: sigma2.1  = sigma2.2')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: sigma2.1 != sigma2.2')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: sigma2.1  > sigma2.2')\n",
    "    elif alternative == 'less':\n",
    "        print('H1: sigma2.1  < sigma2.2')\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        F_samp, p_value, F_crit_L, F_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(F_samp, p_value, F_crit_L, F_crit_R)\n",
    "    print(80*'.')\n",
    "    \n",
    "    # Perform F-test with convention that F = (expected) largest s2 / (expected) smallest s2 (Miller & Miller)\n",
    "    \n",
    "    print('     using convention that F = (expected) largest s2 / (expected) smallest s2:')   \n",
    "    if alternative == 'two-sided':\n",
    "        if (s2_1 > s2_2):\n",
    "            s2_max = s2_1; nu_1 = n_1 - 1\n",
    "            s2_min = s2_2; nu_2 = n_2 - 1\n",
    "        else:\n",
    "            s2_max = s2_2; nu_1 = n_2 - 1\n",
    "            s2_min = s2_1; nu_2 = n_1 - 1\n",
    "        F_samp = s2_max / s2_min\n",
    "        p_value = 2*(1-f.cdf(F_samp, nu_1, nu_2))\n",
    "        # better precision: use survival function\n",
    "        p_value = 2*f.sf(F_samp, nu_1, nu_2)\n",
    "        F_crit_L = float('NaN')\n",
    "        F_crit_R = f.ppf(1-alpha/2, nu_1, nu_2)\n",
    "    if alternative == 'greater':       \n",
    "        F_samp = s2_1 / s2_2\n",
    "        nu_1 = n_1 - 1; nu_2 = n_2 - 1\n",
    "        p_value = 1-f.cdf(F_samp, nu_1, nu_2)\n",
    "        # better precision: use survival function\n",
    "        p_value = f.sf(F_samp, nu_1, nu_2)\n",
    "        F_crit_L = float('NaN')\n",
    "        F_crit_R = f.ppf(1-alpha, nu_1, nu_2)\n",
    "    if alternative == 'less':\n",
    "        F_samp = s2_2 / s2_1\n",
    "        nu_1 = n_2 - 1; nu_2 = n_1 - 1\n",
    "        p_value = 1-f.cdf(F_samp, nu_1, nu_2)\n",
    "        # better precision: use survival function\n",
    "        p_value = f.sf(F_samp, nu_1, nu_2)\n",
    "        F_crit_L = float('NaN')\n",
    "        F_crit_R = f.ppf(1-alpha, nu_1, nu_2)\n",
    "    print('F = {:.4g}, p-value = {:.4g}, F.crit.L = {:.4g}, F.crit.R = {:.4g}, df.1 = {:.4g}, df.2 = {:.4g}'.format(F_samp, p_value, F_crit_L, F_crit_R, nu_1, nu_2))\n",
    "    print(80*'.')\n",
    "    \n",
    "    # Perform F-test with convention that F = s2.1 / s2.2\n",
    "    \n",
    "    print('     using convention that F = s2.1/s2.2:')\n",
    "    nu_12 = n_1 - 1\n",
    "    nu_22 = n_2 - 1\n",
    "    F_samp2 = s2_1 / s2_2\n",
    "    if alternative == 'two-sided':\n",
    "        p_value2 = 2*np.min([f.cdf(F_samp2, nu_12, nu_22), f.sf(F_samp2, nu_12, nu_22)])\n",
    "        F_crit_L2 = f.ppf(alpha/2, nu_12, nu_22)\n",
    "        F_crit_R2 = f.ppf(1-alpha/2, nu_12, nu_22)\n",
    "    if alternative == 'greater':\n",
    "        p_value2 = 1-f.cdf(F_samp2, nu_12, nu_22)\n",
    "        # better precision: use survival function\n",
    "        p_value2 = f.sf(F_samp2, nu_12, nu_22)\n",
    "        F_crit_L2 = 0\n",
    "        F_crit_R2 = f.ppf(1-alpha, nu_12, nu_22)\n",
    "    if alternative == 'less':\n",
    "        p_value2 = f.cdf(F_samp2, nu_12, nu_22)\n",
    "        F_crit_L2 = f.ppf(alpha, nu_12, nu_22)\n",
    "        F_crit_R2 = float('inf')\n",
    "    print('F = {:.4g}, p-value = {:.4g}, F.crit.L = {:.4g}, F.crit.R = {:.4g}, df.1 = {:.4g}, df.2 = {:.4g}'.format(F_samp2, p_value2, F_crit_L2, F_crit_R2, nu_12, nu_22))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(F_samp, p_value, F_crit_L, F_crit_R)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Levene-Brown-Forsythe test for 2 variances (1- and 2-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DS_2sample_Levenetest_vars(y1, y2, center = 'median', alternative = 'two-sided', alpha = 0.05):\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_2sample_Levenetest_vars(y1, y2, center='median', alternative='two-sided', alpha=0.05)\n",
    "     \n",
    "       This function performs a 2-sample Levene-test (Null Hypothesis Significance Test)\n",
    "       in the spirit of R, testing 2 variances without using a normal approximation. \n",
    "    \n",
    "    Requires:          -\n",
    "    \n",
    "    Usage:             DS_2sample_Levenetest_vars(y1, y2, center = ['median']/'mean', \n",
    "                                         alternative=['two-sided']/'less'/'greater', alpha=0.05)\n",
    "    \n",
    "                         center = 'median' [default]          use z.ik = |y.ik - median(y.i)| (Brown-Forsythe)\n",
    "                                  'mean'                      use z.ik = |y.ik - mean(y.i)|   (Levene)\n",
    "                         alternative = 'two-sided' [default]  H1: sigma_1^2 != sigma_2^2\n",
    "                                       'less'                 H1: sigma_1^2 < sigma_2^2\n",
    "                                       'greater'              H1: sigma_1^2 > sigma_2^2\n",
    "                         alpha:   significance level of test [default: 0.05]\n",
    "     \n",
    "    Return:            t, p-value, t.crit.L, t.crit.R  [ + print interpretable output to stdout ]\n",
    "                       where t.crit.L and t.crit.R are the lower and upper critical values, \n",
    "                       t is the test statistic and p-value is the p-value of the test.    \n",
    "     \n",
    "    Author:            M.E.F. Apol\n",
    "    Date:              2022-02-01, rev. 2022_08_26\n",
    "    Validation:        2022-02-01 against scipy.statst.levene and https://www.socscistatistics.com/\n",
    "    \"\"\"\n",
    "     \n",
    "    from scipy.stats import t as t_dist\n",
    "    import numpy as np\n",
    "        \n",
    "    n_1 = len(y1);               n_2 = len(y2)\n",
    "    y_av_1  = np.mean(y1);    y_av_2 = np.mean(y2)\n",
    "    y_med_1 = np.median(y1); y_med_2 = np.median(y2)\n",
    "    s2_1 = np.var(y1, ddof=1);  s2_2 = np.var(y2, ddof=1) \n",
    "    \n",
    "    print(80*'-')\n",
    "    if center == 'median':\n",
    "        print('Brown-Forsythe test for 2 variances:')\n",
    "        print('     using the median of both datasets as center')\n",
    "        y_c_1 = y_med_1; y_c_2 = y_med_2\n",
    "    elif center == 'mean':\n",
    "        print('Levene test for 2 variances:')\n",
    "        print('     using the mean of both datasets as center')\n",
    "        \n",
    "        y_c_1 = y_av_1; y_c_2 = y_av_2\n",
    "    else:\n",
    "        print('Wrong center option...')\n",
    "        print(80*'-' + '\\n')\n",
    "        t_samp, p_value, t_crit_L, t_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(t_samp, p_value, t_crit_L, t_crit_R)\n",
    "    \n",
    "    print('s2.1 = {:.3g}, s2.2 = {:.3g}, n.1 = {:d}, n.2 = {:d}, alpha = {:.3g}'.format(s2_1, s2_2, n_1, n_2, alpha))\n",
    "    print('H0: sigma.1  = sigma.2')\n",
    "    if alternative == 'two-sided':\n",
    "        print('H1: sigma.1 != sigma.2')\n",
    "    elif alternative == 'greater':\n",
    "        print('H1: sigma.1  > sigma.2')\n",
    "    elif alternative == 'less':\n",
    "        print('H1: sigma.1  < sigma.2')\n",
    "    else:\n",
    "        print('Wrong alternative hypothesis chosen!')\n",
    "        print(80*'-' + '\\n')\n",
    "        t_samp, p_value, t_crit_L, t_crit_R = np.nan, np.nan, np.nan, np.nan\n",
    "        return(t_samp, p_value, t_crit_L, t_crit_R)\n",
    "    \n",
    "    z_1 = np.abs(y1 - y_c_1)\n",
    "    z_2 = np.abs(y2 - y_c_2)\n",
    "    z_av_1 = np.mean(z_1);        z_av_2 = np.mean(z_2)\n",
    "    s2_z_1 = np.var(z_1, ddof=1); s2_z_2 = np.var(z_2, ddof=1)\n",
    "    df = n_1 + n_2 - 2\n",
    "    s2_pooled = ((n_1 - 1)*s2_z_1 + (n_2 - 1)*s2_z_2)/df\n",
    "    t_samp = (z_av_1 - z_av_2) / np.sqrt(s2_pooled * (1/n_1 + 1/n_2))\n",
    "    F = t_samp**2\n",
    "    \n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2*(t_dist.cdf(-np.abs(t_samp), df))\n",
    "        t_crit_L = t_dist.ppf(alpha/2, df)\n",
    "        t_crit_R = t_dist.ppf(1-alpha/2, df)\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1-t_dist.cdf(t_samp, df)\n",
    "        # better precision: use survival function\n",
    "        p_value = t_dist.sf(t_samp, df)\n",
    "        t_crit_L = float('-inf')\n",
    "        t_crit_R = t_dist.ppf(1-alpha, df)\n",
    "    elif alternative == 'less':\n",
    "        p_value = t_dist.cdf(t_samp, df)\n",
    "        t_crit_L = t_dist.ppf(alpha, df)\n",
    "        t_crit_R = float('inf')\n",
    "    print('t = {:.4g}, p-value = {:.4g}, t.crit.L = {:.4g}, t.crit.R = {:.4g}, F = {:.4g}'.format(t_samp, p_value, t_crit_L, t_crit_R, F))\n",
    "    print(80*'-' + '\\n')\n",
    "    return(t_samp, p_value, t_crit_L, t_crit_R)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Make a 1d or 2d contingency table from 1 or 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_xtab(*cols, apply_wt=False):\n",
    "    \n",
    "    ################################################################################\n",
    "    #\n",
    "    # Define a function to generate a (1-way or) 2-way contingency table from\n",
    "    # (a single or) two arrays of equal length.\n",
    "    #   assuming a normal approximation\n",
    "    #\n",
    "    # Author:            Doug Ybarbo, adaptation M.E.F. Apol\n",
    "    # Source:            https://gist.github.com/alexland/d6d64d3f634895b9dc8e\n",
    "    # Date:              2021-11-30\n",
    "    # Usage:             my_xtab(y1 [, y2]) \n",
    "    #\n",
    "    #                    Note: y is an array with BINARY data (0, 1)\n",
    "    #\n",
    "    # Return:            uv, table\n",
    "    #                    where\n",
    "    #                       uv = tuple containing the row and column headers\n",
    "    #                       table = contingency table\n",
    "    #\n",
    "    ################################################################################\n",
    "    '''\n",
    "  Source: https://gist.github.com/alexland/d6d64d3f634895b9dc8e  (Doug Ybarbo)\n",
    "  Adapted by M.E.F. Apol, 2021-11-30\n",
    "  returns:\n",
    "    (i) xt, numpy array storing the xtab results, number of dimensions is equal to \n",
    "        the len(args) passed in\n",
    "    (ii) unique_vals_all_cols, a tuple of 1D numpy array for each dimension \n",
    "        in xt (for a 2D xtab, the tuple comprises the row and column headers)\n",
    "    pass in:\n",
    "      (i) 1 or more 1D numpy arrays of integers\n",
    "      (ii) if wts is True, then the last array in cols is an array of weights\n",
    "      \n",
    "  if return_inverse=True, then np.unique also returns an integer index \n",
    "  (from 0, & of same len as array passed in) such that, uniq_vals[idx] gives the original array passed in\n",
    "  higher dimensional cross tabulations are supported (eg, 2D & 3D)\n",
    "  cross tabulation on two variables (columns):\n",
    "  >>> q1 = NP.array([7, 8, 8, 8, 5, 6, 4, 6, 6, 8, 4, 6, 6, 6, 6, 8, 8, 5, 8, 6])\n",
    "  >>> q2 = NP.array([6, 4, 6, 4, 8, 8, 4, 8, 7, 4, 4, 8, 8, 7, 5, 4, 8, 4, 4, 4])\n",
    "  >>> uv, xt = xtab(q1, q2)\n",
    "  >>> uv\n",
    "    (array([4, 5, 6, 7, 8]), array([4, 5, 6, 7, 8]))\n",
    "  >>> xt\n",
    "    array([[2, 0, 0, 0, 0],\n",
    "           [1, 0, 0, 0, 1],\n",
    "           [1, 1, 0, 2, 4],\n",
    "           [0, 0, 1, 0, 0],\n",
    "           [5, 0, 1, 0, 1]], dtype=uint64)\n",
    "    '''\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    if not all(len(col) == len(cols[0]) for col in cols[1:]):\n",
    "        raise ValueError(\"all arguments must be same size\")\n",
    "\n",
    "    if len(cols) == 0:\n",
    "        raise TypeError(\"xtab() requires at least one argument\")\n",
    "\n",
    "    fnx1 = lambda q: len(q.squeeze().shape)\n",
    "    if not all([fnx1(col) == 1 for col in cols]):\n",
    "        raise ValueError(\"all input arrays must be 1D\")\n",
    "\n",
    "    if apply_wt:\n",
    "        cols, wt = cols[:-1], cols[-1]\n",
    "    else:\n",
    "        wt = 1\n",
    "\n",
    "    uniq_vals_all_cols, idx = zip( *(np.unique(col, return_inverse=True) for col in cols) )\n",
    "    shape_xt = [uniq_vals_col.size for uniq_vals_col in uniq_vals_all_cols]\n",
    "    dtype_xt = 'float' if apply_wt else 'uint'\n",
    "    xt = np.zeros(shape_xt, dtype=dtype_xt)\n",
    "    np.add.at(xt, idx, wt)\n",
    "    return uniq_vals_all_cols, xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Beta-Negative Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beta-Negative Binomial distribution has pmf:\n",
    "\n",
    "\\begin{equation}\n",
    "f(y | r, a, b) = \\frac{\\Gamma(r+y)}{\\Gamma(r) \\cdot y!} \\cdot \\frac{B(a+r, b+y)}{B(a, b)}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "y! = \\Gamma(y+1)\n",
    "\\end{equation}\n",
    "\n",
    "so, for computational stability, use\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln (f(y)) = \\ln \\Gamma(r+y) - \\ln \\Gamma(r) - \\ln \\Gamma(y+1) + \\ln B(a+r, b+y) - \\ln B(a,b)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\ln \\Gamma(a)$ is `gammaln(a)` and $\\ln B(a,b)$ is `betaln(a,b)` from `scipy.special`, and calculate $f(y) = e^{ \\ln (f(y) )}$.\n",
    "\n",
    "For the cdf $F(x)$ simply sum all the pmf-values from $y=0$ to $y=x$.\n",
    "\n",
    "Random variables can be generated from random Beta and random Negative Binomial variables:\n",
    "\n",
    "\\begin{equation}\n",
    "p \\sim Beta(a, b) \\\\\n",
    "y \\sim NBin(r, p)\n",
    "\\end{equation}\n",
    "\n",
    "where $p$ is generated using `beta.rvs(a, b, size)` and $y$ is generated using `nbinom.rvs(r, p, size)` from `scipy.stats`.\n",
    "\n",
    "For the quantile (ppf) function, use the `scipy.stats` definition for discrete distributions, that the quantile $Q_p$ is the smallest value $Q_p$ such that $F(Q_p) \\geq p$. Default value for $p=0$ is $Q_p = -1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln \n",
    "from scipy.special import betaln \n",
    "from scipy.stats import beta\n",
    "from scipy.stats import nbinom\n",
    "import numpy as np\n",
    "\n",
    "class DS_Beta_NBinom:\n",
    "    \"\"\"\n",
    "    *\n",
    "    Function DS_Beta_NBinom\n",
    "    \n",
    "       This function calculates properties of the Beta-Negative Binomial distribution\n",
    "       Beta-NBin(r, a, b), in absense of a similar function in scipy.stats...\n",
    "       \n",
    "       Methods:\n",
    "       --------\n",
    "       DS_Beta_NBinom(r, a, b).pmf(y)       probability mass function f(y)\n",
    "       DS_Beta_NBinom(r, a, b).cdf(x)       cumulative distribution function F(x)\n",
    "       DS_Beta_NBinom(r, a, b).ppf(p)       quantile function Q(p) = F^(-1)(p)\n",
    "       DS_Beta_NBinom(r, a, b).rvs(size)    n = size random variables\n",
    "       \n",
    "       Author: M. Emile F. Apol\n",
    "       Date:   2022-11-22\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, r, a, b):\n",
    "        \n",
    "        self.r = r\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "        pass;\n",
    "    \n",
    "        \n",
    "    def pmf(self, y):\n",
    "        \"\"\"\n",
    "        * \n",
    "        Function DS_Beta_NBinom(r, a, b).pmf(y)\n",
    "        \n",
    "           This function calculates the probability mass function f(y) of the \n",
    "           Beta-Negative Binomial Beta-NBin(r, a, b) distribution. \n",
    "           \n",
    "           Return: f(y) value\n",
    "           \n",
    "           Author: M. Emile F. Apol\n",
    "           Date:   2022-08-31, rev. 2022-11-22\n",
    "        \"\"\"\n",
    "        r = self.r; a = self.a; b = self.b\n",
    "        \n",
    "        lnf = gammaln(r+y) - gammaln(r) - gammaln(y+1) + betaln(a+r, b+y) - betaln(a, b)\n",
    "        f = np.exp(lnf)\n",
    "        return(f)\n",
    "    \n",
    "    \n",
    "    def cdf(self, x):\n",
    "        \"\"\"\n",
    "        * \n",
    "        Function DS_Beta_NBinom(r, a, b).cdf(x)\n",
    "        \n",
    "           This function calculates the cumulative probability distribution F(x) of the \n",
    "           Beta-Negative Binomial Beta-NBin(r, a, b) distribution.\n",
    "           \n",
    "           Return: F(x) value\n",
    "           \n",
    "           Author: M. Emile F. Apol\n",
    "           Date:   2022-08-31, rev. 2022-11-22\n",
    "        \"\"\"\n",
    "        r = self.r; a = self.a; b = self.b\n",
    "        \n",
    "        x_help = np.arange(0, x+1)\n",
    "        f_help = np.array([self.pmf(xi) for xi in x_help])\n",
    "        cdf = np.sum(f_help)\n",
    "        return(cdf)\n",
    "    \n",
    "    \n",
    "    def rvs(self, size=1):\n",
    "        \"\"\"\n",
    "        * \n",
    "        Function DS_Beta_NBinom(r, a, b).rvs(size)\n",
    "        \n",
    "           This function calculates n = size random variables from a \n",
    "           Beta-Negative Binomial Beta-NBin(r, a, b) distribution. \n",
    "           \n",
    "           Return: 1d array of values\n",
    "           \n",
    "           Author: M. Emile F. Apol\n",
    "           Date:   2022-08-31, rev. 2022-11-22\n",
    "        \"\"\"\n",
    "        r = self.r; a = self.a; b = self.b\n",
    "        \n",
    "        p = beta.rvs(a, b, loc=0, scale=1, size=size)\n",
    "        x = np.array([nbinom.rvs(r, pii, loc=0, size=1) for pii in p])\n",
    "        # Flatten the resulting array to a list\n",
    "        x = np.concatenate(x).ravel().tolist()\n",
    "        return(x)\n",
    "    \n",
    "\n",
    "    def ppf(self, p):\n",
    "        \"\"\"\n",
    "        * \n",
    "        Function DS_Beta_NBinom(r, a, b).ppf(p)\n",
    "        \n",
    "           This function calculates the quantile Q.p = F^(-1)(p) from a \n",
    "           Beta-Negative Binomial Beta-NBin(r, a, b) distribution. \n",
    "           \n",
    "           Return: quantile Q.p\n",
    "           \n",
    "           Author: M. Emile F. Apol\n",
    "           Date:   2022-08-31, rev. 2022-11-22\n",
    "        \"\"\"\n",
    "        r = self.r; a = self.a; b = self.b\n",
    "        \n",
    "        # First check whether p is a list or a single value\n",
    "        bFloat = bInt = False\n",
    "        if type(p) is float:\n",
    "            p = [p]\n",
    "            bFloat = True\n",
    "        elif type(p) is int:\n",
    "            p = [p]\n",
    "            bInt = True\n",
    "        elif type(p) is list:\n",
    "            p = p\n",
    "        else:\n",
    "            return()\n",
    "        # Do a loop over all values of p\n",
    "        quantile = list()\n",
    "        for pii in p:\n",
    "            # Convention in scipy.stats: if p = 0, x = -1\n",
    "            x = -1\n",
    "            if pii==0.0:\n",
    "                res = x\n",
    "                quantile.append(res)\n",
    "            else:\n",
    "                bDoStep = True\n",
    "                while bDoStep:\n",
    "                    x = x + 1\n",
    "                    F = self.cdf(x)\n",
    "                    if F < pii:\n",
    "                        bDoStep = True\n",
    "                    else:\n",
    "                        bDoStep = False\n",
    "                res = x\n",
    "                quantile.append(res)\n",
    "        if bFloat or bInt:\n",
    "            quantile = quantile[0]\n",
    "        return(quantile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.  Analysis of usual polynomial calibration models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_CalibrationAnalysis:\n",
    "    \n",
    "    def __init__(self, x, y, bVerbatim=True):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Function DS_CalibrationAnalysis(x, y)\n",
    "         \n",
    "           Analyze all 2nd order polynomial calibration models one by one, and collect relevant info.\n",
    "          \n",
    "           Models are:                                   model_id:\n",
    "               Model 0:   y = a.0                        \"0\"\n",
    "               Model 1:   y = a.0 + a.1*x                \"1\"\n",
    "               Model 1a:  y =       a.1*x                \"1a\"\n",
    "               Model 2:   y = a.0 + a.1*x + a.2*x^2      \"2\"\n",
    "               Model 2a:  y =       a.1*x + a.2*x^2      \"2a\"\n",
    "               Model 2b:  y =               a.2*x^2      \"2b\"\n",
    "               Model 2c:  y = a.0         + a.2*x^2      \"2c\"\n",
    "        \n",
    "           Input:\n",
    "               x, y                  independent factor (feature), respons, both 1D-arrays\n",
    "               bVerbatim             True: produce a scatterplot of the data\n",
    "               \n",
    "           Output:\n",
    "               object with several methods:\n",
    "               \n",
    "               .fit()                        fit all models to the data x,y\n",
    "               .summary()                    give a tabular summary of the various calibration models \n",
    "                                             (R2, AIC.c etc.)\n",
    "               .results_model(model_id)      retrieve the OLS results of model model_id\n",
    "               .plot_model(model_id, ...)    plot the predictions of model model_id with CI together\n",
    "                                             with the data; other plotting options are available.\n",
    "               .predict(x_p, model_id, confidence)\n",
    "                                             predict at value x_p a new y_p value with CI\n",
    "               .interpolate(y_0, model_id, confidence)\n",
    "                                             interpolate the value(s) y_0 back to x_0 value with CI \n",
    "                                             \n",
    "            Requires:     -\n",
    "            Loads:        numpy, statsmodels.api, scipy.stats, matplotlib.pyplot\n",
    "               \n",
    "            Author:       M.Emile F. Apol\n",
    "            Date:         2022-09-14, revisions 2022-11-29\n",
    "        \"\"\"\n",
    "        \n",
    "        import numpy as np\n",
    "        import statsmodels.api as sm\n",
    "        from scipy.stats import t\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # To be sure:\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Make libraries available within class:\n",
    "        self.np = np\n",
    "        self.sm = sm\n",
    "        self.t = t\n",
    "        self.plt = plt\n",
    "        \n",
    "        # Make data available within class:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            # For convenience, produce a preliminary scatter plot:\n",
    "            plt.scatter(x, y, marker='o', color='black', label='Experimental')\n",
    "            plt.xlabel('Independent factor, $x$')\n",
    "            plt.ylabel('Response, $y$')\n",
    "            plt.legend(loc='best')\n",
    "            plt.title('Calibration data')\n",
    "            plt.show()\n",
    "        \n",
    "        pass;\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, bVerbatim=True):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.fit()\n",
    "        \n",
    "           Fit all calibration models to the x,y data, and collect info.\n",
    "        \n",
    "           Input:\n",
    "              bVerbatim               True : print output of each model to stdout\n",
    "                                      False: do not print\n",
    "        \n",
    "           Author:          M.Emile F. Apol\n",
    "           Date:            2022-09-21\n",
    "        \"\"\"\n",
    "        \n",
    "        sm = self.sm\n",
    "        np = self.np\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        \n",
    "        def DS_AIC(fit):\n",
    "    \n",
    "        ###########################################################################\n",
    "        #\n",
    "        # Calculate the AIC and AIC.c-values of a LS linear regression model\n",
    "        # based on the 'fit' object from an OLS fit using statsmodels.api\n",
    "        #\n",
    "        # Equations:\n",
    "        #\n",
    "        # AIC = n*log(SS.err/n) + 2*(P+1)\n",
    "        # AIC.c = n*log(SS.err/n) + 2*(P+1) + 2*(P+1)*(P+2)/(n-P-2)\n",
    "        # \n",
    "        # with P the number of parameters a.i in the regression model.\n",
    "        #\n",
    "        # Author:        M.E.F. Apol\n",
    "        # Date:          2020-01-06\n",
    "        #\n",
    "        # Return:        AIC, AIC.c\n",
    "        #\n",
    "        ###########################################################################\n",
    "    \n",
    "            SS_err = fit.ssr\n",
    "            n = fit.nobs\n",
    "            P = len(fit.params)\n",
    "            AIC = n * np.log(SS_err/n) + 2 * (P + 1)\n",
    "            if(n-P-2 > 0):\n",
    "                AIC_c = AIC + 2 * (P + 1) * (P + 2) / (n - P - 2)\n",
    "            else:\n",
    "                AIC_c = np.nan\n",
    "            return(AIC, AIC_c);\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        # Analyze all calibration models one by one:\n",
    "        ###########################\n",
    "        \n",
    "        Models = [\"0\",\"1\",\"1a\",\"2\",\"2a\",\"2b\",\"2c\"]\n",
    "        Formulas = [\"y = a.0\",\"y = a.0 + a.1*x\", \"y = a.1*x\", \"y = a.0 + a.1*x + a.2*x^2\", \n",
    "                    \"y = a.1*x + a.2*x^2\", \"y = a.2*x^2\", \"y = a.0 + a.2*x^2\"]\n",
    "        Mod_d = {\n",
    "            \"0\": 0,\n",
    "            \"1\": 1,\n",
    "            \"1a\": 2,\n",
    "            \"2\": 3,\n",
    "            \"2a\": 4,\n",
    "            \"2b\": 5,\n",
    "            \"2c\": 6\n",
    "        }\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print()\n",
    "            print('Analyzing all calibration models:')\n",
    "            print(\"-\"*80)\n",
    "        \n",
    "        ###########################\n",
    "        # Model 0: y = a.0\n",
    "        ###########################\n",
    "        \n",
    "        n = len(x)\n",
    "        X_0 = np.ones(n)\n",
    "        model = sm.OLS(y,X_0)\n",
    "        results_0 = model.fit()\n",
    "        a_0_0 = results_0.params[0]; a_1_0 = np.nan; a_2_0 = np.nan\n",
    "        s_a_0_0 = results_0.bse[0]; s_a_1_0 = np.nan; s_a_2_0 = np.nan\n",
    "        p_a_0_0 = results_0.pvalues[0]; p_a_1_0 = np.nan; p_a_2_0 = np.nan\n",
    "        R2_0 = 0.0; R2_adj_0 = 0.0\n",
    "        AIC_0, AIC_c_0 = DS_AIC(results_0)\n",
    "        s2_yx_0 = results_0.scale\n",
    "        V_0 = results_0.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"0\"]] + \": \" + Formulas[Mod_d[\"0\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_0, a_1_0, a_2_0))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_0, p_a_1_0, p_a_2_0))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_0, R2_adj_0))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_0, AIC_c_0))\n",
    "            print(\"-\"*80)\n",
    "        \n",
    "        ###########################\n",
    "        # Model 1: y = a.0 + a.1*x\n",
    "        ###########################\n",
    "        \n",
    "        # First, add the constant a_0 to the model:\n",
    "        X_1 = sm.add_constant(x)\n",
    "        model = sm.OLS(y,X_1)\n",
    "        results_1 = model.fit()\n",
    "        a_0_1 = results_1.params[0]; a_1_1 = results_1.params[1]; a_2_1 = np.nan\n",
    "        s_a_0_1 = results_1.bse[0]; s_a_1_1 = results_1.bse[1]; s_a_2_1 = np.nan\n",
    "        p_a_0_1 = results_1.pvalues[0]; p_a_1_1 = results_1.pvalues[1]; p_a_2_1 = np.nan\n",
    "        R2_1 = results_1.rsquared; R2_adj_1 = results_1.rsquared_adj\n",
    "        AIC_1, AIC_c_1 = DS_AIC(results_1)\n",
    "        s2_yx_1 = results_1.scale\n",
    "        V_1 = results_1.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"1\"]] + \": \" + Formulas[Mod_d[\"1\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_1, a_1_1, a_2_1))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_1, p_a_1_1, p_a_2_1))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_1, R2_adj_1))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_1, AIC_c_1))\n",
    "            print(\"-\"*80)\n",
    "        \n",
    "        ###########################\n",
    "        # Model 1a: y = a.1*x\n",
    "        ###########################\n",
    "        \n",
    "        # First, do not add the constant a_0 to the model:\n",
    "        X_1a = x\n",
    "        model = sm.OLS(y,X_1a)\n",
    "        results_1a = model.fit()\n",
    "        a_0_1a = np.nan; a_1_1a = results_1a.params[0]; a_2_1a = np.nan\n",
    "        s_a_0_1a = np.nan; s_a_1_1a = results_1a.bse[0]; s_a_2_1a = np.nan\n",
    "        p_a_0_1a = np.nan; p_a_1_1a = results_1a.pvalues[0]; p_a_2_1a = np.nan\n",
    "        R2_1a = results_1a.rsquared; R2_adj_1a = results_1a.rsquared_adj\n",
    "        AIC_1a, AIC_c_1a = DS_AIC(results_1a)\n",
    "        s2_yx_1a = results_1a.scale\n",
    "        V_1a = results_1a.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"1a\"]] + \": \" + Formulas[Mod_d[\"1a\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_1a, a_1_1a, a_2_1a))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_1a, p_a_1_1a, p_a_2_1a))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_1a, R2_adj_1a))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_1a, AIC_c_1a))\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "        ###########################\n",
    "        # Model 2: y = a.0 + a.1*x + a.2*x^2\n",
    "        ###########################\n",
    "        \n",
    "        # First, add the x^2 column and a constant a_0 to the model:\n",
    "        X_2 = np.column_stack((x, x**2))\n",
    "        X_2 = sm.add_constant(X_2)\n",
    "        model = sm.OLS(y,X_2)\n",
    "        results_2 = model.fit()\n",
    "        a_0_2 = results_2.params[0]; a_1_2 = results_2.params[1]; a_2_2 = results_2.params[2]\n",
    "        s_a_0_2 = results_2.bse[0]; s_a_1_2 = results_2.bse[1]; s_a_2_2 = results_2.bse[2]\n",
    "        p_a_0_2 = results_2.pvalues[0]; p_a_1_2 = results_2.pvalues[1]; p_a_2_2 = results_2.pvalues[2]\n",
    "        R2_2 = results_2.rsquared; R2_adj_2 = results_2.rsquared_adj\n",
    "        AIC_2, AIC_c_2 = DS_AIC(results_2)\n",
    "        s2_yx_2 = results_2.scale\n",
    "        V_2 = results_2.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"2\"]] + \": \" + Formulas[Mod_d[\"2\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_2, a_1_2, a_2_2))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_2, p_a_1_2, p_a_2_2))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_2, R2_adj_2))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_2, AIC_c_2))\n",
    "            print(\"-\"*80)\n",
    "           \n",
    "        ###########################\n",
    "        # Model 2a: y = a.1*x + a.2*x^2\n",
    "        ###########################\n",
    "        \n",
    "        # First, add the x^2 column but do not add a constant a_0 to the model:\n",
    "        X_2a = np.column_stack((x, x**2))\n",
    "        model = sm.OLS(y,X_2a)\n",
    "        results_2a = model.fit()\n",
    "        a_0_2a = np.nan; a_1_2a = results_2a.params[0]; a_2_2a = results_2a.params[1]\n",
    "        s_a_0_2a = np.nan; s_a_1_2a = results_2a.bse[0]; s_a_2_2a = results_2a.bse[1]\n",
    "        p_a_0_2a = np.nan; p_a_1_2a = results_2a.pvalues[0]; p_a_2_2a = results_2a.pvalues[1]\n",
    "        R2_2a = results_2a.rsquared; R2_adj_2a = results_2a.rsquared_adj\n",
    "        AIC_2a, AIC_c_2a = DS_AIC(results_2a)\n",
    "        s2_yx_2a = results_2a.scale\n",
    "        V_2a = results_2a.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"2a\"]] + \": \" + Formulas[Mod_d[\"2a\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_2a, a_1_2a, a_2_2a))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_2a, p_a_1_2a, p_a_2_2a))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_2a, R2_adj_2a))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_2a, AIC_c_2a))\n",
    "            print(\"-\"*80)\n",
    "        \n",
    "        ###########################\n",
    "        # Model 2b: y = a.2*x^2\n",
    "        ###########################\n",
    "        \n",
    "        # First, make the x^2 column but do not add a constant a_0 to the model:\n",
    "        X_2b = x**2\n",
    "        model = sm.OLS(y,X_2b)\n",
    "        results_2b = model.fit()\n",
    "        a_0_2b = np.nan; a_1_2b = np.nan; a_2_2b = results_2b.params[0]\n",
    "        s_a_0_2b = np.nan; s_a_1_2b = np.nan; s_a_2_2b = results_2b.bse[0]\n",
    "        p_a_0_2b = np.nan; p_a_1_2b = np.nan; p_a_2_2b = results_2b.pvalues[0]\n",
    "        R2_2b = results_2b.rsquared; R2_adj_2b = results_2b.rsquared_adj\n",
    "        AIC_2b, AIC_c_2b = DS_AIC(results_2b)\n",
    "        s2_yx_2b = results_2b.scale\n",
    "        V_2b = results_2b.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"2b\"]] + \": \" + Formulas[Mod_d[\"2b\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_2b, a_1_2b, a_2_2b))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_2b, p_a_1_2b, p_a_2_2b))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_2b, R2_adj_2b))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_2b, AIC_c_2b))\n",
    "            print(\"-\"*80)\n",
    "        \n",
    "        ###########################\n",
    "        # Model 2c: y = a.0 + a.2*x^2\n",
    "        ###########################\n",
    "        \n",
    "        # First, make the x^2 column and a constant a_0 to the model:\n",
    "        X_2c = x**2\n",
    "        X_2c = sm.add_constant(X_2c)\n",
    "        model = sm.OLS(y,X_2c)\n",
    "        results_2c = model.fit()\n",
    "        a_0_2c = results_2c.params[0]; a_1_2c = np.nan; a_2_2c = results_2c.params[1]\n",
    "        s_a_0_2c = results_2c.bse[0]; s_a_1_2c = np.nan; s_a_2_2c = results_2c.bse[1]\n",
    "        p_a_0_2c = results_2c.pvalues[0]; p_a_1_2c = np.nan; p_a_2_2c = results_2c.pvalues[1]\n",
    "        R2_2c = results_2c.rsquared; R2_adj_2c = results_2c.rsquared_adj\n",
    "        AIC_2c, AIC_c_2c = DS_AIC(results_2c)\n",
    "        s2_yx_2c = results_2c.scale\n",
    "        V_2c = results_2c.cov_params()\n",
    "        \n",
    "        if(bVerbatim):\n",
    "            print('Model ' + Models[Mod_d[\"2c\"]] + \": \" + Formulas[Mod_d[\"2c\"]])\n",
    "            print('Parameters:  a.0 = {:.3g}, a.1 = {:.3g}, a.2 = {:.3g}'.format(a_0_2c, a_1_2c, a_2_2c))\n",
    "            print('P-values:    p(a.0) = {:.3g}, p(a.1) = {:.3g}, p(a.2) = {:.3g}'.format(p_a_0_2c, p_a_1_2c, p_a_2_2c))\n",
    "            print('Effect size: R2 = {:.5f}, R2.adj = {:.5f}'.format(R2_2c, R2_adj_2c))\n",
    "            print('AIC = {:.2f}, AIC.c = {:.2f}'.format(AIC_2c, AIC_c_2c))\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        # Collect relevant info in self:\n",
    "        ###########################\n",
    "        \n",
    "        self.n = n\n",
    "        \n",
    "        self.results_0 = results_0; self.V_0 = V_0  # general results; var-covar par matrix\n",
    "        self.results_1 = results_1; self.V_1 = V_1\n",
    "        self.results_1a = results_1a; self.V_1a = V_1a\n",
    "        self.results_2 = results_2; self.V_2 = V_2\n",
    "        self.results_2a = results_2a; self.V_2a = V_2a\n",
    "        self.results_2b = results_2b; self.V_2b = V_2b\n",
    "        self.results_2c = results_2c; self.V_2c = V_2c\n",
    "        \n",
    "        self.Models = Models # names of models\n",
    "        self.Mod_d = Mod_d # model dictionary\n",
    "        self.Formulas = Formulas # formulas of models\n",
    "        self.Ps = [1, 2, 1, 3, 2, 1, 2] # number of parameters of model\n",
    "        self.AICs = [AIC_0, AIC_1, AIC_1a, AIC_2, AIC_2a, AIC_2b, AIC_2c]\n",
    "        self.AIC_cs = [AIC_c_0,AIC_c_1, AIC_c_1a, AIC_c_2, AIC_c_2a, AIC_c_2b, AIC_c_2c]\n",
    "        self.R2s = [R2_0, R2_1, R2_1a, R2_2, R2_2a, R2_2b, R2_2c]\n",
    "        self.a_0s = [a_0_0, a_0_1, a_0_1a, a_0_2, a_0_2a, a_0_2b, a_0_2c]\n",
    "        self.a_1s = [a_1_0, a_1_1, a_1_1a, a_1_2, a_1_2a, a_1_2b, a_1_2c]\n",
    "        self.a_2s = [a_2_0, a_2_1, a_2_1a, a_2_2, a_2_2a, a_2_2b, a_2_2c]\n",
    "        self.s_a_0s = [s_a_0_0, s_a_0_1, s_a_0_1a, s_a_0_2, s_a_0_2a, s_a_0_2b, s_a_0_2c]\n",
    "        self.s_a_1s = [s_a_1_0, s_a_1_1, s_a_1_1a, s_a_1_2, s_a_1_2a, s_a_1_2b, s_a_1_2c]\n",
    "        self.s_a_2s = [s_a_2_0, s_a_2_1, s_a_2_1a, s_a_2_2, s_a_2_2a, s_a_2_2b, s_a_2_2c]\n",
    "        self.p_a_0s = [p_a_0_0, p_a_0_1, p_a_0_1a, p_a_0_2, p_a_0_2a, p_a_0_2b, p_a_0_2c]\n",
    "        self.p_a_1s = [p_a_1_0, p_a_1_1, p_a_1_1a, p_a_1_2, p_a_1_2a, p_a_1_2b, p_a_1_2c]\n",
    "        self.p_a_2s = [p_a_2_0, p_a_2_1, p_a_2_1a, p_a_2_2, p_a_2_2a, p_a_2_2b, p_a_2_2c]\n",
    "        self.Xs = [X_0, X_1, X_1a, X_2, X_2a, X_2b, X_2c]\n",
    "        self.s2_yxs = [s2_yx_0, s2_yx_1, s2_yx_1a, s2_yx_2, s2_yx_2a, s2_yx_2b, s2_yx_2c]\n",
    "        \n",
    "        pass;\n",
    "\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.summary()\n",
    "          \n",
    "           Print a summary of all models in tabular form:\n",
    "         \n",
    "                Model_id, formula, AIC.c, Delta.c, w, R2, p-values parameters\n",
    "        \n",
    "        where  \n",
    "                AIC.c        = small-sample Akaike Information Criterion\n",
    "                Delta.c      = AIC.c (model) - lowest AIC.c-value\n",
    "                w            = Akaike weight of model: w.m = exp(-Delta.m/2)/sum(exp(-Delta.j/2))_(j=1)^M\n",
    "                R2           = R2-value (effect size of coefficient of determination)\n",
    "        \n",
    "           Author:      M.Emile F. Apol\n",
    "           Date:        2022-09-14, revisions 2022-11-29\n",
    "        \"\"\"\n",
    "        \n",
    "        print('\\n'+'='*80)\n",
    "        print('SUMMARY of Calibration Models')\n",
    "        print('='*80 + '\\n')\n",
    "        \n",
    "        Models = self.Models\n",
    "        Mod_d = self.Mod_d\n",
    "        Formulas = self.Formulas\n",
    "        AIC_cs = self.AIC_cs\n",
    "        min_AIC_c = np.min(AIC_cs)\n",
    "        Delta_cs = AIC_cs - min_AIC_c\n",
    "        ws = np.exp(-0.5*Delta_cs)     # Akaike weights per model\n",
    "        ws_tot = np.sum(ws)\n",
    "        ws /= ws_tot\n",
    "        R2s = self.R2s\n",
    "        p_a_0s = self.p_a_0s\n",
    "        p_a_1s = self.p_a_1s\n",
    "        p_a_2s = self.p_a_2s\n",
    "        \n",
    "        # model formula AIC.c Delta.c w.c R2 p(a0) p(a1) p(a2)\n",
    "        print_str1 = \"{:<10} {:<30} {:^10} {:^10} {:^10} {:^10} {:^10} {:^10} {:^10}\"\n",
    "        print_str2 = \"{:<10} {:<30} {:>+10.2f} {:>10.2f} {:^10.3f} {:<10.5f} {:<10.3g} {:<10.3g} {:<10.3g}\"\n",
    "        \n",
    "        print(print_str1.format('Model','Formula','AIC.c','Delta.c','w', 'R2','p(a.0)','p(a.1)','p(a.2)'))\n",
    "        print(print_str1.format('----------','------------------------------','----------','----------','----------',\n",
    "                                '----------','----------','----------','----------'))\n",
    "        print(print_str2.format(Models[Mod_d[\"0\"]],Formulas[Mod_d[\"0\"]],AIC_cs[Mod_d[\"0\"]],Delta_cs[Mod_d[\"0\"]],ws[Mod_d[\"0\"]],R2s[Mod_d[\"0\"]],p_a_0s[Mod_d[\"0\"]],p_a_1s[Mod_d[\"0\"]],p_a_2s[Mod_d[\"0\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"1\"]],Formulas[Mod_d[\"1\"]],AIC_cs[Mod_d[\"1\"]],Delta_cs[Mod_d[\"1\"]],ws[Mod_d[\"1\"]],R2s[Mod_d[\"1\"]],p_a_0s[Mod_d[\"1\"]],p_a_1s[Mod_d[\"1\"]],p_a_2s[Mod_d[\"1\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"1a\"]],Formulas[Mod_d[\"1a\"]],AIC_cs[Mod_d[\"1a\"]],Delta_cs[Mod_d[\"1a\"]],ws[Mod_d[\"1a\"]],R2s[Mod_d[\"1a\"]],p_a_0s[Mod_d[\"1a\"]],p_a_1s[Mod_d[\"1a\"]],p_a_2s[Mod_d[\"1a\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"2\"]],Formulas[Mod_d[\"2\"]],AIC_cs[Mod_d[\"2\"]],Delta_cs[Mod_d[\"2\"]],ws[Mod_d[\"2\"]],R2s[Mod_d[\"2\"]],p_a_0s[Mod_d[\"2\"]],p_a_1s[Mod_d[\"2\"]],p_a_2s[Mod_d[\"2\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"2a\"]],Formulas[Mod_d[\"2a\"]],AIC_cs[Mod_d[\"2a\"]],Delta_cs[Mod_d[\"2a\"]],ws[Mod_d[\"2a\"]],R2s[Mod_d[\"2a\"]],p_a_0s[Mod_d[\"2a\"]],p_a_1s[Mod_d[\"2a\"]],p_a_2s[Mod_d[\"2a\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"2b\"]],Formulas[Mod_d[\"2b\"]],AIC_cs[Mod_d[\"2b\"]],Delta_cs[Mod_d[\"2b\"]],ws[Mod_d[\"2b\"]],R2s[Mod_d[\"2b\"]],p_a_0s[Mod_d[\"2b\"]],p_a_1s[Mod_d[\"2b\"]],p_a_2s[Mod_d[\"2b\"]]))\n",
    "        print(print_str2.format(Models[Mod_d[\"2c\"]],Formulas[Mod_d[\"2c\"]],AIC_cs[Mod_d[\"2c\"]],Delta_cs[Mod_d[\"2c\"]],ws[Mod_d[\"2c\"]],R2s[Mod_d[\"2c\"]],p_a_0s[Mod_d[\"2c\"]],p_a_1s[Mod_d[\"2c\"]],p_a_2s[Mod_d[\"2c\"]]))\n",
    "        pass;\n",
    "    \n",
    "    \n",
    "   \n",
    "    def results_model(self, model):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.results_model(model)\n",
    "          \n",
    "           Select the regression results of the requested calibration model model_id:\n",
    "         \n",
    "           Input:\n",
    "              model                   preferred model: string [\"0\", \"1\", \"1a\", \"2\", \"2a\", \"2b\", \"2c\"]\n",
    "              \n",
    "           Returns:\n",
    "              OLS object with results of the regression\n",
    "          \n",
    "           Author:      M. Emile F. Apol\n",
    "           Date:        2022-09-15\n",
    "        \"\"\"\n",
    "        \n",
    "        if model == \"0\":\n",
    "            res = self.results_0\n",
    "        elif model == \"1\":\n",
    "            res = self.results_1\n",
    "        elif model == \"1a\":\n",
    "            res = self.results_1a\n",
    "        elif model == \"2\":\n",
    "            res = self.results_2\n",
    "        elif model == \"2a\":\n",
    "            res = self.results_2a\n",
    "        elif model == \"2b\":\n",
    "            res = self.results_2b\n",
    "        elif model == \"2c\":\n",
    "            res = self.results_2c\n",
    "        else:\n",
    "            print(\"Wrong model chosen!\")\n",
    "            res = None\n",
    "            return()\n",
    "        \n",
    "        return(res)\n",
    "    \n",
    "    \n",
    "    def predict(self, xp, model, confidence=0.95, bVerbatim=False, bDebug=False):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.predict(xp, model, confidence=0.95, bVerbatim=False, bDebug=False)\n",
    "        \n",
    "          Predict y.p of the requested calibration model with \"confidence\"*100% CI \n",
    "          for given x.p value\n",
    "         \n",
    "          General formula:\n",
    "         \n",
    "             s_yp^2 = xp^T * V * xp\n",
    "         \n",
    "             CI(yp) = t(n-P) * s_yp\n",
    "         \n",
    "          Reference: Montgomery & Runger (2011), Eq. (12-39)\n",
    "          \n",
    "          Input:\n",
    "             xp                      value of x for prediction\n",
    "             model                   preferred model: string [\"0\", \"1\", \"1a\", \"2\", \"2a\", \"2b\", \"2c\"]\n",
    "             confidence              [default: 0.95]\n",
    "             bVerbatim               True: print results to stdout\n",
    "         \n",
    "          Returns:                   yp, yp_lower, yp_upper\n",
    "          \n",
    "          Author:     M. Emile F. Apol\n",
    "          Date:       2022-09-15\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the shared libraries:\n",
    "        t = self.t\n",
    "        np = self.np\n",
    "        \n",
    "        Err = 0\n",
    "        \n",
    "        # Get the required shared data:\n",
    "        n = self.n\n",
    "        Ps = self.Ps\n",
    "        Models = self.Models\n",
    "        Mod_d = self.Mod_d\n",
    "        a_0s = self.a_0s\n",
    "        a_1s = self.a_1s\n",
    "        a_2s = self.a_2s\n",
    "        \n",
    "        if((confidence >= 0) & (confidence <=1)):\n",
    "            alpha = 1 - confidence\n",
    "            conf_str = \"{:g}\".format(100*confidence)\n",
    "        else:\n",
    "            print('Wrong confidence chosen! Must be between 0 and 1...')\n",
    "            Err = 1;\n",
    "            return()\n",
    "        \n",
    "        if(Err != 1):\n",
    "            \n",
    "            if(model==\"0\"):\n",
    "                a = np.array([a_0s[Mod_d[\"0\"]]])\n",
    "                V = self.V_0\n",
    "                Xp = np.array([1])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"0\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "        \n",
    "            elif(model==\"1\"):\n",
    "                a = np.array([a_0s[Mod_d[\"1\"]], a_1s[Mod_d[\"1\"]]])\n",
    "                V = self.V_1\n",
    "                Xp = np.array([1, xp])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"1\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            elif(model==\"1a\"):\n",
    "                a = np.array([a_1s[Mod_d[\"1\"]]])\n",
    "                V = self.V_1a\n",
    "                Xp = np.array([xp])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"1a\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            elif(model==\"2\"):\n",
    "                a = np.array([a_0s[Mod_d[\"2\"]], a_1s[Mod_d[\"2\"]], a_2s[Mod_d[\"2\"]]])\n",
    "                V = self.V_2\n",
    "                Xp = np.array([1, xp, xp**2])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"2\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            elif(model==\"2a\"):\n",
    "                a = np.array([a_1s[Mod_d[\"2a\"]], a_2s[Mod_d[\"2a\"]]])\n",
    "                V = self.V_2a\n",
    "                Xp = np.array([xp, xp**2])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"2a\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            elif(model==\"2b\"):\n",
    "                a = np.array([a_2s[Mod_d[\"2b\"]]])\n",
    "                V = self.V_2b\n",
    "                Xp = np.array([xp**2])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"2b\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            elif(model==\"2c\"):\n",
    "                a = np.array([a_0s[Mod_d[\"2c\"]], a_2s[Mod_d[\"2c\"]]])\n",
    "                V = self.V_2c\n",
    "                Xp = np.array([1, xp**2])\n",
    "                yp = np.dot(a, Xp)\n",
    "                s_yp = np.sqrt(np.dot(np.matmul(Xp, V), Xp))\n",
    "                df = n-Ps[Mod_d[\"2c\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                yp_lower = yp - t_val*s_yp\n",
    "                yp_upper = yp + t_val*s_yp\n",
    "                if(bDebug):\n",
    "                    print(\"a: \", a)\n",
    "                    print(\"x: \", Xp)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"yp: {:.4f}, CI = [{:.4f}, {:.4f}]\".format(yp, yp_lower, yp_upper))\n",
    "            \n",
    "            else:\n",
    "                print(\"Wrong model chosen!\")\n",
    "                Err = 1\n",
    "                return()\n",
    "            \n",
    "        if(Err==1):\n",
    "            return()\n",
    "        else:\n",
    "            if(bVerbatim):\n",
    "                print(\"Predicted value at x = {:.4g}:\".format(xp))\n",
    "                print(\"yp = {:.4g}, \".format(yp) + conf_str + \"% CI = [{:.4g}, {:.4g}]\".format(yp_lower, yp_upper))\n",
    "                \n",
    "            return(yp, yp_lower, yp_upper);\n",
    "    \n",
    "    \n",
    "            \n",
    "    def plot_model(self, model, confidence=0.95, **kwargs):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.plot_model(self, model, confidence=0.95, **kwargs)\n",
    "          \n",
    "           Make a plot of the experimental data with the requested calibration model and CI.\n",
    "         \n",
    "           Input:\n",
    "              model                   preferred model: string [\"0\", \"1\", \"1a\", \"2\", \"2a\", \"2b\", \"2c\"]\n",
    "              confidence              confidence level [default: 0.95]\n",
    "          \n",
    "           Optional arguments:\n",
    "              xlabel, ylabel          labels along the x- and y-axis [def: \"Independent factor, x\", \"Response, y\"]\n",
    "              title                   Main title above the plot [def: Calibration curve]\n",
    "              xlim, ylim              lists [x_min, x_max] and/or [y_min, y_max] minimum \n",
    "                                              and maximum values along the axes\n",
    "              data_color, data_marker color and type of marker for experimental points [def: 'black', 'o']\n",
    "              line_color, line_style  line color and style for calibration model [def: \"red\", '-']\n",
    "              CI_color, CI_style      line color and style for CI lines [def: 'blue', '--']\n",
    "          \n",
    "           Author:      M.Emile F. Apol\n",
    "           Date:        2022-09-22\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the libraries:\n",
    "        plt = self.plt\n",
    "        t = self.t\n",
    "        \n",
    "        Err = 0\n",
    "        \n",
    "        if((confidence >= 0) & (confidence <=1)):\n",
    "            alpha = 1 - confidence\n",
    "        else:\n",
    "            print('Wrong confidence chosen! Must be between 0 and 1...')\n",
    "            Err = 1;\n",
    "        \n",
    "        # First, get possible extra plotting arguments:\n",
    "        # xlabel, ylabel, title, xlim, ylim, data_color, data_marker, line_color\n",
    "        xlabel = kwargs.get('xlabel', 'Independent factor, $x$')\n",
    "        ylabel = kwargs.get('ylabel', 'Response, $y$')\n",
    "        title = kwargs.get('title', 'Calibration curve')\n",
    "        xlim = kwargs.get('xlim', None)\n",
    "        ylim = kwargs.get('ylim', None)\n",
    "        data_color = kwargs.get('data_color', 'black')\n",
    "        data_marker = kwargs.get('data_marker', 'o')\n",
    "        line_color = kwargs.get('line_color', 'red')\n",
    "        line_style = kwargs.get('line_style', '-')\n",
    "        CI_color = kwargs.get('CI_color', 'blue')\n",
    "        CI_style = kwargs.get('CI_style', '--')\n",
    "        \n",
    "        # get the relevant info:\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        n = self.n\n",
    "        Ps = self.Ps\n",
    "        Models = self.Models\n",
    "        Mod_d = self.Mod_d\n",
    "        \n",
    "        if xlim is None:\n",
    "            x_min = np.min(x); x_max = np.max(x)\n",
    "        else:\n",
    "            x_min = xlim[0]\n",
    "            x_max = xlim[1]\n",
    "\n",
    "        # Plot the best model in a graph:\n",
    "        \n",
    "        n_plot = 101\n",
    "        xn = np.linspace(x_min, x_max, n_plot)\n",
    "        y_pred  = []\n",
    "        y_lower = []\n",
    "        y_upper = []\n",
    "        \n",
    "        # Calculate predictions per model:\n",
    "        \n",
    "        for xi in xn:\n",
    "            y_temp, lower_temp, upper_temp = self.predict(xi, model=model, confidence=1-alpha, bVerbatim=False)\n",
    "            y_pred.append(y_temp)\n",
    "            y_lower.append(lower_temp)\n",
    "            y_upper.append(upper_temp)\n",
    "            \n",
    "        # CHECK FOR WRONG MODEL!\n",
    "        \n",
    "        # Generic plotting of data and model:\n",
    "        if(Err != 1):\n",
    "            conf_str = \"{:g}\".format(100*confidence)\n",
    "            plt.scatter(x, y, marker=data_marker, color=data_color, label='Experimental')\n",
    "            plt.plot(xn, y_pred, color=line_color, linestyle=line_style, \n",
    "                     label='Model ' + Models[Mod_d[model]])\n",
    "            plt.plot(xn, y_lower, color=CI_color, linestyle=CI_style, label=conf_str + \"% CI\")\n",
    "            plt.plot(xn, y_upper, color=CI_color, linestyle=CI_style)\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.legend(loc='best')\n",
    "            plt.title(title)\n",
    "            plt.xlim(xlim)\n",
    "            plt.ylim(ylim)\n",
    "            plt.show()\n",
    "\n",
    "        pass;\n",
    "    \n",
    "         \n",
    "    def interpolate(self, y_0, model, confidence=0.95, bVerbatim=True, bDebug = False, **kwargs):\n",
    "        \"\"\"\n",
    "        *\n",
    "        Method DS_CalibrationAnalysis.interpolate(y_0, model, confidence=0.95, bVerbatim=True, bDebug = False, **kwargs)\n",
    "          \n",
    "           Interpolate measured y.0-values (m replicas) with the requested calibration model \n",
    "           towards an x.0-value, with standard error and CI.\n",
    "         \n",
    "           General formula:\n",
    "         \n",
    "               s_x0^2 = (s_yx^2 / m) * (dx0/dy0)^2 + (dx0/da)^T * V * (dx0/da)\n",
    "         \n",
    "               CI(x0) = t(n-P) * s_x0\n",
    "          \n",
    "           Input:\n",
    "               y_0                     array of (replica) measurements of y-values of a single sample\n",
    "               model                   preferred model: string [\"0\", \"1\", \"1a\", \"2\", \"2a\", \"2b\", \"2c\"]\n",
    "               confidence              confidence level [default: 0.95]\n",
    "               bVerbatim               True: print results also to stdout\n",
    "                                       False: no output to stdout\n",
    "          \n",
    "           Optional arguments:\n",
    "         \n",
    "           Return:                     x_0_star, x_0_lower, x_0_upper, s_x_0, df, y_0_av, m\n",
    "          \n",
    "           Author:          M. Emile F. Apol\n",
    "           Date:            2022-09-25\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the libraries:\n",
    "        plt = self.plt\n",
    "        t = self.t\n",
    "        np = self.np\n",
    "        \n",
    "        # Get the relevant info:\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        n = self.n\n",
    "        Ps = self.Ps\n",
    "        Models = self.Models\n",
    "        Mod_d = self.Mod_d  \n",
    "        a_0s = self.a_0s\n",
    "        a_1s = self.a_1s\n",
    "        a_2s = self.a_2s\n",
    "        s2_yxs = self.s2_yxs\n",
    "        Xs = self.Xs\n",
    "        \n",
    "        # Check if y_0 is an array, if a scaler make it an array:\n",
    "        if(not isinstance(y_0, (np.ndarray, list))):\n",
    "            y_0 = np.array([y_0])\n",
    "        else:\n",
    "            y_0 = np.array(y_0)\n",
    "        \n",
    "        Err = 0\n",
    "        \n",
    "        if((confidence >= 0) & (confidence <=1)):\n",
    "            alpha = 1 - confidence\n",
    "        else:\n",
    "            print('Wrong confidence chosen! Must be between 0 and 1...')\n",
    "            Err = 1;\n",
    "            return()\n",
    "        \n",
    "        if(Err != 1):\n",
    "            \n",
    "            # Determine the middle of the calibration x-data:\n",
    "            xc_min = np.min(x)\n",
    "            xc_max = np.max(x)\n",
    "            xc_mid = (xc_max - xc_min)/2\n",
    "            \n",
    "            # Determine properties of replica y0-values:\n",
    "            y_0 = np.array(y_0)     # to be sure\n",
    "            m = len(y_0)            # nr of replicas\n",
    "            y_0_av = np.mean(y_0)   # mean measured respons of sample\n",
    "            \n",
    "            # Do interpolation per model:\n",
    "        \n",
    "            if(model==\"1\"):\n",
    "                a_0 = a_0s[Mod_d[\"1\"]]; a_1 = a_1s[Mod_d[\"1\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"1\"]]\n",
    "                x_0 = (y_0_av - a_0) / a_1\n",
    "                x_0_star = x_0\n",
    "                V = self.V_1\n",
    "                dx0dy0 = (1 / a_1)\n",
    "                dx0da = (-1 / a_1) * np.array([1, x_0_star])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"1\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "                \n",
    "            \n",
    "            elif(model==\"1a\"):\n",
    "                a_1 = a_1s[Mod_d[\"1a\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"1a\"]]\n",
    "                x_0 = (y_0_av) / a_1\n",
    "                x_0_star = x_0\n",
    "                V = self.V_1a\n",
    "                dx0dy0 = (1 / a_1)\n",
    "                dx0da = (-1 / a_1) * np.array([x_0_star])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"1a\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "            \n",
    "            elif(model==\"2\"):\n",
    "                a_0 = a_0s[Mod_d[\"2\"]]; a_1 = a_1s[Mod_d[\"2\"]]; a_2 = a_2s[Mod_d[\"2\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"2\"]]\n",
    "                x_0_1 = (-a_1 + np.sqrt(a_1**2 + 4*a_2*(y_0_av-a_0)))/(2*a_2)\n",
    "                x_0_2 = (-a_1 - np.sqrt(a_1**2 + 4*a_2*(y_0_av-a_0)))/(2*a_2)\n",
    "                if(np.abs(x_0_1-xc_mid) < np.abs(x_0_2-xc_mid)):\n",
    "                    x_0_star = x_0_1\n",
    "                else:\n",
    "                    x_0_star = x_0_2\n",
    "                V = self.V_2\n",
    "                dx0dy0 = (1 / (a_1+2*a_2*x_0_star))\n",
    "                dx0da = (-1 / (a_1+2*a_2*x_0_star)) * np.array([1, x_0_star, x_0_star**2])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"2\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "            \n",
    "            elif(model==\"2a\"):\n",
    "                a_1 = a_1s[Mod_d[\"2a\"]]; a_2 = a_2s[Mod_d[\"2a\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"2a\"]]\n",
    "                x_0_1 = (-a_1 + np.sqrt(a_1**2 + 4*a_2*(y_0_av)))/(2*a_2)\n",
    "                x_0_2 = (-a_1 - np.sqrt(a_1**2 + 4*a_2*(y_0_av)))/(2*a_2)\n",
    "                if(np.abs(x_0_1-xc_mid) < np.abs(x_0_2-xc_mid)):\n",
    "                    x_0_star = x_0_1\n",
    "                else:\n",
    "                    x_0_star = x_0_2\n",
    "                V = self.V_2a\n",
    "                dx0dy0 = (1 / (a_1+2*a_2*x_0_star))\n",
    "                dx0da = (-1 / (a_1+2*a_2*x_0_star)) * np.array([x_0_star, x_0_star**2])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"2a\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "            \n",
    "            elif(model==\"2b\"):\n",
    "                a_2 = a_2s[Mod_d[\"2b\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"2b\"]]\n",
    "                x_0_1 = np.sqrt(y_0_av / a_2)\n",
    "                x_0_2 = -1*np.sqrt(y_0_av / a_2)\n",
    "                if(np.abs(x_0_1-xc_mid) < np.abs(x_0_2-xc_mid)):\n",
    "                    x_0_star = x_0_1\n",
    "                else:\n",
    "                    x_0_star = x_0_2\n",
    "                V = self.V_2b\n",
    "                dx0dy0 = (1 / (2*a_2*x_0_star))\n",
    "                dx0da = (-1 / (2*a_2*x_0_star)) * np.array([x_0_star**2])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"2b\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "            \n",
    "            elif(model==\"2c\"):\n",
    "                a_0 = a_0s[Mod_d[\"2c\"]]; a_2 = a_2s[Mod_d[\"2c\"]]\n",
    "                s2_yx = s2_yxs[Mod_d[\"2c\"]]\n",
    "                x_0_1 = np.sqrt((y_0_av-a_0) / a_2)\n",
    "                x_0_2 = -1*np.sqrt((y_0_av-a_0) / a_2)\n",
    "                if(np.abs(x_0_1-xc_mid) < np.abs(x_0_2-xc_mid)):\n",
    "                    x_0_star = x_0_1\n",
    "                else:\n",
    "                    x_0_star = x_0_2\n",
    "                V = self.V_2c\n",
    "                dx0dy0 = (1 / (2*a_2*x_0_star))\n",
    "                dx0da = (-1 / (2*a_2*x_0_star)) * np.array([1, x_0_star**2])\n",
    "                s_x_0 = np.sqrt((s2_yx / m) * dx0dy0**2 + np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                df = n-Ps[Mod_d[\"2c\"]]\n",
    "                t_val = t.ppf(1-alpha/2, df)\n",
    "                x_0_lower = x_0_star - t_val*s_x_0\n",
    "                x_0_upper = x_0_star + t_val*s_x_0\n",
    "            \n",
    "            else:\n",
    "                print(\"Wrong model chosen!\")\n",
    "                Err = 1\n",
    "                return()\n",
    "            \n",
    "            if(bDebug and (Err !=1)):\n",
    "                    print(\"y0: \", y_0_av, \", m: \",m)\n",
    "                    print(\"s2_yx: \", s2_yx)\n",
    "                    print(\"dx0dy0: \", dx0dy0)\n",
    "                    print(\"dx0da:  \", dx0da)\n",
    "                    print(\"V: \", V)\n",
    "                    print(\"V: \", V_alt)\n",
    "                    print(\"x0: \", x_0_star)\n",
    "                    print(\"s2_x0 (1): \", (s2_yx / m) * dx0dy0**2)\n",
    "                    print(\"s2_x0 (2): \", np.dot(np.matmul(dx0da, V), dx0da))\n",
    "                    print(\"df: \", df)\n",
    "                    print(\"x0: {:.4f}, 95% CI = [{:.4f}, {:.4f}]\".format(x_0_star, x_0_lower, x_0_upper))\n",
    "            \n",
    "            # print results to stdout:\n",
    "            if(bVerbatim):\n",
    "                conf_str = \"{:g}\".format(100*confidence)\n",
    "                print(\"Interpolation of model \" + model + \":\")\n",
    "                print('y.0 = {:.4g}, m = {:d} replica(s)'.format(y_0_av, m))\n",
    "                print('x.0 = {:.4g}, '.format(x_0_star) + \n",
    "                     conf_str + \"% CI = [{:.4g}, {:.4g}]\".format(x_0_lower, x_0_upper))\n",
    "                print('s.x.0 = {:.4g}'.format(s_x_0))\n",
    "            \n",
    "            \n",
    "        if(Err==1):\n",
    "            return()\n",
    "        else:\n",
    "            return(x_0_star, x_0_lower, x_0_upper, s_x_0, df, y_0_av, m);\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
